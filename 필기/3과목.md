# 📌 3과목

## #분석모형 설계

### 1. 분석 절차 수립

- **과대적합** : 모델이 지나치게 데이터를 학습해 매우 복잡해진 모형
- 과소적합 : 데이터를 충분히 설명하지 못하는 단순한 모델

### 2. 분석 환경 구축

- 분석 도구 선정
    - R : 통계 분석에 특화, 처리속도 느림, 강력한 시각화
    - Python : 간결함, 높은 가독성, R보다 빠른 속도, R보다 약한 시각화

- 데이터 분할 : 과대적합/과소적합 방지 및 데이터 불균형 문제 해결
    - 훈련용(Training) : 모델 학습 - *50%*
    - 검증용(Validation) - *30%*
    - 평가용(Test) : 모델 평가 - *20%*

## #분석기법 적용

### 1. 분석기법

- **회귀분석** : 독립변수가 종속변수에 미치는 영향을 파악
    - 잔차 : 실제값과 예측값의 차이 (오차 = 모집단 기준, 잔차 = 표본집단 기준)
    - 회귀계수 추정방법 : **최소제곱법**
        - 잔차의 제곱합이 최소가 되는 회귀계수와 절편을 구하는 방법
    - 회귀모형 평가 : R-square (0~1 사이 값)
- 선형회귀분석의 가정
    1. 선형성 : 종속-독립변수는 선형관계
    2. 독립성 : 잔차와 독립변수 간 상관관계가 없음
        
        **다중공선성* : 독립변수 서로 간에 강한 상관관계가 나타나는 문제
        
        → 차원 축소, 상관계수 분해를 통해 해결 
        
    3. 정상성(정규성) : 잔차가 정규분포의 특성을 지님
    4. 등분산성 : 잔차의 분산이 고르게 분포
    5. 비상관성 : 잔차들끼리 상관이 없어야 함
- 회귀 모형 변수 선택 방법
    - 전진선택법
    - 후진선택법
    - 단계별 선택법 : 전진+후진 / 변수 추가 시 벌점(AIC, BIC) 고려

- **로지스틱 회귀분석** : 종속변수가 범주형 데이터일 때 활용
    - 오즈 : 성공 확률과 실패 확률의 비
    - 로짓(logit) 변환 : 오즈에 자연로그를 취하는 방법
        
        → 독립변수가 n 증가하면 확률이 e의 n승 만큼 증가
        

- **의사결정나무** : 여러 개 분리 기준으로 최종 분류 값을 찾음
    - 분류에서의 분할 방법 : CHAID, CART(지니지수), C4.5/C5.0(엔트로피지수)
    - 회귀에서의 분할 방법 : CHAID, CART
    - 정지규칙 : 분리를 더 이상 수행하지 않게 함
    - 가지치기 : 일부 가지를 제거하여 과적합 방지

- **인공신경망** : 인간의 뇌 구조를 모방한 퍼셉트론 활용
    - 다중 퍼셉트론 - 입력층~출력층 사이에 1개 이상의 은닉층 보유
        
        *은닉층 수는 사용자가 직접 설정
        
- **활성화 함수** : 인공신경망의 선형성 극복 (XOR 문제 해결)
    - 시그모이드 함수 : 0~1 사이 값을 가지며 로지스틱 회귀분석과 유사
    - 소프트맥스 함수 : 목표 데이터가 다범주인 경우 각 범주에 속할 사후확률을 제공
    - 하이퍼볼릭 탄젠트 함수 : 시그모이드의 중심 값을 0으로 이동 → -1~1 사이 값
    - ReLU 함수 : 기울기 소실문제 극복 → 0 또는 1 둘 중 하나 값
- 인공신경망 과적합 방지방안
    - 규제 - **라쏘(L1)** 규제 : 맨하탄 거리 기반(절대값), **릿지(L2)** 규제 : 유클리드 거리 기반(제곱)
    - 드롭아웃 - 일부 퍼셉트론을 비활성화
    - 조기종료
    - 모델의 복잡도 줄이기 - 은닉층의 퍼셉트론 감소
    - 데이터 증강
    - 배치정규화
- 인공신경망 학습 방법
    - 역전파 알고리즘 : 가중치를 수정하여 오차를 줄임 (출력층 → 입력층)
    - 경사하강법 : 기울기를 낮은 쪽으로 이동시켜 극값(global minimum)에 이를 때까지 반복

- **서포트벡터머신(SVM)** : 마진이 최대가 되는 **초평면**을 찾아 분류
    - 초평면(하이퍼플레인) : 데이터를 구분하는 기준이 되는 경계
    - 서포트벡터 : 클래스를 나누는 초평면과 가까운 위치의 샘플
    - 마진 : 하이퍼플레인과 서포트벡터 사이의 거리
    - **커널함수** : 저차원 데이터를 고차원 데이터로 변경하는 함수
- SVM 유형
    1. 하드마진분류 : 오류 비허용
    2. 소프트마진분류 : 마진 내 어느 정도 오류 허용

- **연관분석** : 항목들 간 조건-결과로 이뤄지는 패턴 발견 기법 (장바구니 분석)
    - 특징
        - 결과가 단순, 분명
        - 품목 수 많을수록 계산이 기하급수적으로 증가
        - Apriori 알고리즘 활용
- 연관분석의 지표 (하단 공식 암기 부분에 기재)
    1. 지지도
    2. 신뢰도
    3. 향상도

- **군집분석** : 비지도 학습, 데이터 간 거리나 유사성을 기준으로 군집 나눔
- 거리측도
    1. 연속형 변수
        - **유클리디안 거리** : 차이의 제곱의 합의 루트
        - **맨하튼 거리** : 차이의 절대값의 합
        - **체비셰프 거리** : 차이의 절대값 중 최댓값
        - 표준화 거리 : 유클리디안 거리를 표준편차로 나눔
        - 민코프스키 거리 : 유클리드/맨하튼 거리를 일반화
        - 마할라노비스 거리 : 표준화 거리에서 변수의 *상관성* 고려
    2. 범주형 변수 - 자카드 유사도, 코사인 유사도
- **계층적** 군집분석
    1. 거리측정 방법 : 최단, 최장, 평균, 중심, 와드
    2. 덴드로그램 (Tree 모양 그래프)
- **비계층적** 군집분석
    1. K평균 군집화
        - 지정된 군집 개수에 따라 평균을 기준으로 중심점 설정 → 중심점 변경 시 군집 변할 수 있음
        - 이상치에 민감 → 이에 대응하기 위해 K-medoids 군집방법 존재
    2. DBSCAN
        - 밀도기반, 군집개수 지정 필요 없음
        - 노이즈 / 이상치에 강함
    3. 기타
        1. 퍼지군집화
        2. EM알고리즘
        3. 자기조직화지도(SOM) : 신경망 활용하여 차원축소(고차원 → 저차원)를 통해 군집화

### 2. 고급 분석기법

- 분할표 : 여러 개 범주형 변수를 기준으로 관측치 기록한 표 (오즈비 계산)

- **다변량 분석**
    
    *⇒ 다변량 데이터들은 분석할 때 차원축소!* 
    
    - 요인분석 : 다수 변수들의 상관관계를 분석하여 소수 요인으로 축약하는 기법
        1. 요인추출방법 : 주성분분석, 공통요인분석
        2. 요인회전

- **시계열 분석** : 시간 흐름에 따라 관찰된 자료 특성을 파악하여 미래 예측
    - **정상성** = 모든 시점에 일정한 평균과 분산을 가져야 함
        
        *차분 : 현 시점의 자료를 이전 값으로 빼는 방법
        
        *자기상관 : 현재 상태가 과거&미래와 밀접한 관련이 있음 = 독립적이지 않음 (시계열 데이터에서의 공분산 기법)
        
    - 백색잡음 : 시계열 모형의 오차항 의미
- 시계열 모형
    1. 자기회귀(AR) 모형 : 자신의 과거 값이 미래를 결정
    2. 이동평균(MA) 모형 : 백색잡음들의 선형결합으로 표현 (관측치에 모두 동일 가중치 부여)
        
        *지수평활법 : MA 종류 중 하나로, 최근 관측치에 더 높은 가중치를 부여 
        
    3. 자기회귀누적이동평균(ARIMA) 모형 = AR + MA
        
        : ARIMA (p, d, q)에서 d는 차분 횟수를 의미
        
- 분해시계열 - 시계열에 영향을 주는 요인을 분리해 해석하는 방법
    1. 추세 요인 : 장기적으로 증가하거나 감소
    2. 계절 요인 : 특정 시기에 나타나는 고정된 주기
    3. 순환(주기, cycle) 요인 : 알려지지 않은 주기, 중장기적
    4. 불규칙 요인 : 설명 불가 요인

- **베이지안 기법**
    - 베이즈 정리 (하단 공식 암기에 기재)
    - 나이브베이즈 분류 = 나이브(독립) + 베이즈 이론

- 인공신경망
    - DNN : 은닉층 2개 이상으로 구성된 인공신경망
    - CNN(합성곱 신경망) : 이미지에서 패턴을 찾음
    - RNN(순환 신경망) : 순차적 데이터(시계열 데이터) 학습에 특화
        - **장기의존성** 문제 ~ 과거 정보가 전달되지 못함
            
            ⇒ **LSTM / GRU** 모델(Reset, Update)로 극복
            

- **오토인코더** : 입력 데이터를 인코더로 압축한 후 디코더로 재구성하는 비지도 학습 신경망

- 텍스트 마이닝
    - 통계적 기반
        1. TDM : 문서에서 등장하는 단어들의 빈도를 행렬로 표현
        2. TF-IDF : 단어 등장 빈도를 특정 문서, 전체 문서에서 비교하는 것
    - 단어 수준 기반
        1. Word2Vec : 거리를 기반으로 하여 벡터로 표현 ~ CBOW, Skip-Gram
        2. FastText : 하나의 단어를 여러 개로 잘라서 벡터로 계산
        3. ELMo : 양방향 언어 모델 적용

- 트랜스포머 : RNN의 느린 속도와 병렬 처리 불가 단점을 개선한 모델 (BERT, GPT)

- **앙상블 분석** : 여러 개의 예측 모형들을 조합 → 전체적인 분산을 감소시켜 성능 향상
    1. 보팅 : 다수결 방식
    2. **배깅** : 복원추출하는 붓스트랩으로 학습 후 보팅으로 결합
    3. **부스팅** : 잘못된 분류 데이터에 큰 가중치를 주는 방법(순차적) ~ 이상치에 민감, 병렬처리 불가
    4. 랜덤포레스트 : 배깅 + 의사결정트리 ⇒ 성능 우수하며 이상치에 강함

- 비모수검정
    - 모집단에 대한 정보가 없을 때, 관측 자료의 분포 가정 불가한 상태일 때
    - 두 관측 간 순위나 차이로 검정
    - 종류 : 부호검정, 순위합검정, 만-휘트니 U검정, 크러스컬-월리스 검정
