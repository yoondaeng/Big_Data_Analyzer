# Big_Data_Analyzer
#### 빅데이터분석기사 필기 요약 정리본입니다.
### 참고
[빅데이터분석기사 필기 강의](https://youtube.com/playlist?list=PLWtr7MRpQi5Dt41ZE0mT_wFIWUsTT1E7O&si=r1faM39OxVWndCJW)

## 분석 과제 우선순위 선정

### 사분면 분석기법

| 어려움 | 1 | 2 |
| --- | --- | --- |
| 쉬움 | 3 | 4 |
| 난이도\시급성 | 현재 | 미래 |
- 1분면: 난이도가 높아 현재 수준에서 과제 적용하기 어려움
- 2분면: 어렵지만 시급성 낮음 → 분석 과제를 바로 적용하기에는 난이도가 높음
- 3분면: 과제 추진 난이도가 높지 않아 우선적으로 바로 적용
- 4분면: 중요도 낮음, 시급성 낮음
- 적용 우선 순위
   - 시급성 `3→4→2`
   - 난이도순 `3→1→2`

## 데이터 분석 절차

문제 인식 → 연구조사 → 모형화 → 데이터 수집 → 데이터 분석 → 분석 결과 제시

## 데이터 확보 계획 수립

- 데이터 확보 계획 수립 시 데이터 수집을 위해 **데이터 적정성**, **가용성,** **대체 분석 데이터 유무**에 대한 확인 필요
- 타당성 검증을 위해 편익/비용 검증과 **기술적 타당성**을 점검

→ 데이터 확보 계획 수립 시 분석 변수 점검 항목으로 데이터 다양성 X

## 원천 데이터

**원천 데이터에 대한 정보**

- 데이터의 수집 가능성, 데이터의 보안, 데이터 정확성, 수집 난이도, 수집 비용 항목

→ 데이터 신속성은 포함되지 않음.

## 분석 마스터 플랜

**우선순위를 결정하는 주요 요인**

- 전략적 중요도
- 비즈니스 성과 (ROI)
- 실행용이성

## 분석작업계획 수립 절차

- 프로젝트 소요비용 배분
- 프로젝트 작업분할구조 수립
- 프로젝트 업무 분장 계획 및 배분

→ 프로젝트 정의서 작성은 분석작업계획 수립 절차에서 실행하지 않음.

**분석 로드맵 설정 시 세부적인 일정계획 수립 방안으로 적절하지 않은 것은?**

1. 데이터 수집과 데이터 준비 단계를 병렬로 진행한다. → 순차
2. 반복적인 정련과정을 통해 프로젝트의 완성도를 높여 나간다.
3. 모델링 단계는 반복적으로 수행하여야 한다.
4. 순차형과 반복형을 혼합하여 사용한다.

**정답 : 1**

데이터 수집 및 확보와 데이터 준비 단계는 순차적으로 진행하여야 한다.

## 정형데이터의 품질 진단 기법

- 메타데이터 수집 및 분석
- 컬럼 속성 분석
- 누락 값 분석
- 값의 허용 범위 분석
- 허용값 목록 분석
- 문자열 패턴 분석
- 날짜 유형 분석
- 기타 특수 도메인 분석
- 유일 값 분석
- 구조 분석

→ 운영환경 호환성 분석 ❌

## 데이터 가치

**다음 중 데이터의 가치 측정이 어려운 이유로 틀린 것은?**

1. 데이터 재사용이 일반화되며 특정 데이터를 누가 언제 사용했는지 알기 어렵다.
2. 분석 기술의 발전으로 과거에는 불가능했던 데이터 분석이 가능해졌다.
3. 한정된 곳에서 데이터가 활용되고 있다.
    
    → 데이터의 가치는 데이터 활용 방식, 재사용, 가치 창출 방식, 분석 기술 발전으로 인해 측정하기 어렵다.
    
4. 기존에 존재하지 않던 새로운 가치를 창출한다

## 빅데이터 플랫폼 등장배경

- 비즈니스 요구사항 변화
- 데이터 처리 복잡도 증가
- 데이터 규모 증가
- 데이터 구조의 변화 → 정형, 비정형, 반정형
- 데이터 처리의 신속성 요구

→ **데이터 처리 유연성 증대는 빅데이터 플랫폼이 등장한 후에 얻어진 결과이지,** 플랫폼이 필요해진 원인은 아님

## 빅데이터 저장시스템

- 선정 기능성 **비교분석** 요소
    - 데이터모델, 확장성, 트랜잭션 일관성, 질의지원, 접근성
- 기존 시스템과의 **연계성** 요소
    - 호환성

## 비식별화된 개인정보의 재식별 가능성 검토 기법

k-익명성, l-다양성, t-근접성

## 데이터유형

- 정형 데이터
    - 정형화된 스키마를 가진 데이터
    - 품질기준
        - 완전성, 유일성, 유효성, 일관성, **정확성**
- 비정형 데이터
    - 이미지나 동영상으로 존재하는 데이터, **TEXT File**
    - 품질 기준
        - 기능성, 신뢰성, 사용성, 효율성, 이식성
- 반정형 데이터
    - 웹로그, 센서데이터, JSON파일

### 시간데이터

- 유효시간: 객체가 발생하거나 소멸된 시간
- 거래시간: 관리 시스템을 통해 처리된 시간
- 스냅샷 데이터: 시간 개념이 필요하지 않아 거래, 유효시간을 미지원하는 데이터
- 이원시간 데이터: 거래, 유효시간과 스냅샷데이터를 동시에 지원

## 질적자료(Qualitative Data)

- 정성적 자료라고도 하며 자료를 범주의 형태로 분류
- 분류의 편리상 부여된 수치의 크기자체에는 의미를 부여하지 않는 자료
- 명목자료, 서열자료 등이 질적자료로 분류
    - 서열자료: 수치나 기호가 서열을 나타내는 자료

### 명목자료

질적자료의 한 종류로 측정대상이 범주나 종류에 대해 구분되어지는 것을 수치 또는 기회로 분류되는 자료

### 서열자료

질적자료의 한 종류로 명목자료와 비슷하나 수치나 기호가 서열을 나타내는 자료

### 구간자료

수치자료의 한 종류

명목 + 서열자료 → 숫자로 표현된 변수에 대해서 변수 간의 관계가 산술적인 의미를 가지는 자료

### 비율자료

수치자료의 한 종류

명목 + 서열 + 구간자료 → 수치화된 변수에 비율의 개념을 도입

### 질적자료(Qualitative Data)와 수치자료(Quantitative Data) 세부 유형

| **자료 유형** | **세부 유형** | **정의** | **특징** | **예시** |
| --- | --- | --- | --- | --- |
| **질적자료 (Qualitative Data)** | **명목자료 (Nominal Data)** | 대상이 서열 없이 단순히 분류된 데이터 | 숫자가 부여될 수 있지만 크기 비교 불가 | 성별 (남/여), 혈액형 (A/B/O/AB), 국적 (한국/미국/중국) |
|  | **서열자료 (Ordinal Data)** | 대상이 순서(서열)가 있는 범주형 데이터 | 크기 비교 가능하지만, 차이의 정도 비교 불가 | 학점 (A, B, C, D, F), 설문조사 (매우 좋음, 좋음, 보통, 나쁨, 매우 나쁨) |
| **수치자료 (Quantitative Data)** | **구간자료 (Interval Data)** | 숫자 간 차이(간격)가 의미 있지만 비율 비교 불가 | 덧셈/뺄셈 가능, 곱셈/나눗셈 불가 | 온도 (°C, °F), IQ 점수 |
|  | **비율자료 (Ratio Data)** | 절대적 0이 존재하며 비율 비교 가능 | 덧셈/뺄셈/곱셈/나눗셈 모두 가능 | 키 (cm), 몸무게 (kg), 연봉 (만원), 나이 (세) |

## 요약변수

(가) 수집된 정보를 분석에 맞게 종합(aggregate)한 변수이다.

(나) 데이터 마트에서 가장 기본적인 변수이다.

(다) 많은 분석 모델에서 공통으로 사용될 수 있어 재활용성이 높다.

특정 상황에만 의미성 부여가 아닌 보편적이고 전 데이터구간에 대표성을 가지는 변수생성을 위해서 노력해야 한다. → 파생 변수의 생성 및 처리 시의 유의점 설명

## 정보획득(Information Gain)

정보이론에서 순도가 증가하고 불확실성이 감소하는 것

## 텍스트 마이닝

- 대규모 문서에서 정보 추출, 연계성 파악, 분류 및 군집화, 요약 등을 통해 데이터에 숨겨진 의미를 발견하는 기법
- 텍스트 마이닝은 인간의 언어로 이루어진 비정형 텍스트 데이터들을 자연어 처리방식을 이용하여 데이터를 처리하는 전처리가 필요하다.

## 오피니언 마이닝

사람들의 주관적인 의견을 통계/수치화하여 객관적인 정보로 바꾸는 기술이다. 어떤 사안이나 인물에 대한 사람들의 의견 뿐만 아니라 감정과 태도도 분석하기 때문에 감정 분석이라고도 불린다.

## 리얼리티 마이닝

통화/메시징 등의 커뮤니케이션 데이터, gps/wifi 등의 위치 데이터이다. 이를 통해 사회적 행위를 마이닝하고 사용자 행동 모델링이나 라이프 로그도 얻어내는 것을 목표로 한다.

## CRISP-DM 분석 방법론 절차

업무 이해(Business Understanding) → 데이터 이해(Data Understanding) → 데이터 준비(Data Preparation) → 모델링(Modeling)→ 평가(Evaluation) → 전개(Deployment)

→ 업무, 데이터 이해를 하고 준비를 한다.

### 평가 단계 프로세스

- 분석 모델 평가

### 전개 프로세스 (최종 단계)

- 전개 계획 수립
- 모니터링 계획 수립
- 프로젝트 종료보고서 작성

C**RISP-DM 방법론의 전개단계를 구성하는 프로세스가 아닌 것은?**

1. 전개계획 수립
2. 모니터링계획 수립
3. 프로젝트 종료보고서 작성
4. 분석 모델 평가

**정답 : 4**

CRISP-DM 방법론에서 분석 모델 평가는 평가단계의 프로세스이다.

## KDD 분석 방법론

데이터셋 선택(Selection) → 데이터 전처리(Preprocessing) → 데이터 변환(Transformation) → 데이터마이닝(Data Mining) → 데이터마이닝 결과 평가(Interpretation/ Evaluation)

## 분석 시나리오 작성

분석과정에 필요한 데이터, 절차, 분석기법 등의 세부사항들을 정의

분석과정과 결과가 어떻게 활용되는지 명확히 알 수 있음.

## 분석 방법론의 구성요건

- 상세한 절차, 방법
- 도구와 기법
- 템플릿과 산출물
- 어느 정도의 지식만 있으면 활용 가능한 수준의 난이도

→ 메뉴얼과 전문지식까지는 필요하지 않음.

## 익명화(Anonymization)

사생활 침해를 방지하기 위하여 데이터에 포함된 개인정보를 삭제하거나 알아볼 수 없는 형태로 변환하는 방법

## **데이터 분석 과제 수행을 위한 필요역량**

- 도메인 이슈 도출
- 분석 목표 수립
- 프로젝트 계획 수립
- 보유 데이터 자산 확인 능력

→ 데이터 거버넌스 체계 수립 역량은 적합하지 않음.

## Date Warehouse

- 구성요소

데이터 모델, ETL, ODS, DW Meta Data, OLAP, 데이터 마이닝, 분석 TOOL, 경영기반 솔루션

## 빅데이터 플랫폼의 빅데이터 수집기술

- 크롤링, 로그수집기, 센서 네트워크, RSS, Reader/Open API, ETL

## 데이터 저장 방식

- 파일 시스템
- 관계형 데이터베이스
- 분산처리 데이터베이스

## 시공간 정의언어

데이터베이스에서 시공간 데이터를 저장할 수 있도록 테이블을 정의하는 언어.

시공간테이블 인덱스 및 뷰의 정의문, 변경문

공간적 속성과 시간적 속성 동시에 포함, 점, 선, 면 등의 공간속성 타입 추가

## 시공간 조작언어

저장된 시공간 데이터를 조작(삽입, 수정, 삭제, 검색)하는 언어.

객체의 삽입, 삭제, 변경 등의 검색문

시간지원 연산자와 공간연산자를 포함 → 객체에 대한 공간관리와 이력정보 제공

# 기술 통계

---

- 데이터를 요약하고 정리하여 특징을 파악
- 평균, 분산, 변동계수 등의 값도출을 통해 분석 대상의 구조적 특이성을 밝혀낸다.
- 데이터의 시각화를 통해서 데이터의 분포모양 구조적 특이성에 대한 가설과 실제 간의 차이를 밝혀낸다.
- 평균, 중앙값, 최빈값, 분산, 표준편차, 사분위수, 히스토그램
- 활용
    - 데이터 시각화, 데이터 탐색적 분석(EDA)

## 탐색적 데이터 분석

수집한 데이터가 들어왔을 때, 다양한 방법을 통해서 자료를 관찰하고 이해하는 과정

### 필요성

- 데이터의 분포 및 값을 검토함으로써 데이터가 표현하는 현상 이해
- 문제점 발견 시 본 분석 전 데이터 수집 의사 결정 가능
- 새로운 양상을 발견 시 초기설정 문제의 가설을 수정하거나 새로운 가설 설립 가능

## 사분위편차

데이터의 변동성을 측정하는 방법

$$
Q3 - Q1
$$

### **사분위수 정의**

- **Q1 (제1사분위수, 25%)**: **하위 25% 지점의 값**
- **Q2 (제2사분위수, 50%)**: **중앙값(Median), 하위 50% 지점의 값**
- **Q3 (제3사분위수, 75%)**: **하위 75% 지점의 값**

**다음 자료에 대해서 사분위편차를 구하시오.**

8, 10, 12, 13, 15, 17, 17, 18, 19, 23, 24 ← 오름차순 정렬, n = 11

**정답 : 7**

### **데이터의 개수 확인**

- 데이터 개수 **n = 11개**

### **2. Q1(제1사분위수) 계산**

- **공식:**Q1=(n+1)×10025=(11+1)×10025=12×0.25=3
    
    $$
    Q1 = (n+1) \times \frac{25}{100}
    $$
    
    $$
    = (11+1) \times \frac{25}{100}
    $$
    
    $$
    12 \times 0.25 = 3
    $$
    
    - **3번째 값 → Q1 = 12**

### **3. Q3(제3사분위수) 계산**

- **공식**
    
    $$
    Q3 = (n+1) \times \frac{75}{100}
    $$
    
    $$
    = (11+1) \times \frac{75}{100}
    $$
    
    $$
     12 \times 0.75 = 9
    $$
    
    - **9번째 값 → Q3 = 19**

### **4. 사분위편차(IQR) 계산**

$$
IQR=Q3−Q1=19−12= 7
$$

## 요인분석

- 다수의 변수들의 상관관계를 분석하여 공통차원을 축약하는 통계분석 과정
- 독립변수, 종속변수 개념이 없음, 주로 **기술 통계**에 의한 방법 이용
- 요인에 대한 중요도를 파악하고 필요가 없다면 제거
- 변수 특성 파악을 위해 관련된 변수들이 묶임으로써 요인 간의 상호 독립성 파악 필요

### 요인분석의 목적

- 변수특성파악: 관련된 변수들의 묶음으로 상호독립특성을 파악하기 용이
- 측정항목의 타당성 평가: 그룹이 되지 않은 변수의 특성 구분
- 요인점수를 통한 변수생성: 희귀분석, 군집분석, 판별분석 등에 적용 가능한 변수 생성

# 추론 통계

---

- 표본을 이용해 모집단의 특성을 추정하고 검정
- 모집단에 대한 어떤 미지의 양상을 알기 위해 통계학을 이용하여 추측하는 과정을 지칭하며 통계학의 한 부분으로서 추론 통계학이라 불린다.
- 추정(estimation)은 표본을 통해 모집단 특성이 어떠한가에 대해 추측하는 과정이다. 표본평균 계산을 통해 모집단평균을 추측해보거나, 모집단 평균에 대한 95% 신뢰구간의 계산 과정을 나타낸다.

## 회귀분석

하나의 종속 변수와 독립 변수 간의 관계를 분석하는 통계 기법

### 가정

- 선형성: 독립변수와 종속변수가 선형적이어야 함
- 잔차 정규성: 잔차의 기댓값은 0이며 정규분포를 이루어야 함
- 잔차 등분산성: 전차들의 분산이 일정
- 다중공산성: 3개 이상의 독립변수간의 상관관계로 인한 문제가 없어야 함.

### 다중회귀분석

- 목적
    - **예측**: 종속 변수의 값을 예측하기 위해 여러 독립 변수를 사용
    - **관계 분석**: 독립 변수들이 종속 변수에 미치는 영향을 파악
- 진행순서
    - 다중공선성 진단 → 회귀계수 유의성 확인 → 수정된 결정계수 확인 → 모형의 적합도 평가
- 기본 가정
    - 회귀모형은 모수에 대해 선형인 모형이다.
    - 종속 변수와 독립 변수들 간의 선형 관계 기반
    - 오차항의 분산은 모든 관찰치에 대해 σ^2의 일정한 분산을 갖는다.
    - 서로 다른 관찰치 간의 오차항은 상관이 없다(오차항은 서로독립이며 공분산은 **0**)
    - 오차항은 정규분포를 따르며 N(0,σ^2)이다.

### **회귀계수**

각 독립 변수들이 종속 변수에 미치는 영향을 나타내는 값

이 값들이 유의미한지 검정하고, 회귀계수의 크기와 부호로 변수들의 관계를 해석할 수 있습니다.

### **다중공선성(Multicollinearity)**

독립 변수들 간에 높은 상관관계가 있을 경우, 다중공선성이 발생

이로 인해 회귀계수 추정치의 신뢰성이 낮아질 수 있습니다.

### **결정계수(R²) 및 수정된 결정계수**

모델이 데이터를 얼마나 잘 설명하는지를 나타냄

결정계수(R²)가 높을수록 모델이 데이터를 잘 설명

수정된 결정계수는 변수 수를 고려하여 결정계수를 보정한 값

### **모형의 적합도 평가**

회귀모델이 실제 데이터를 얼마나 잘 설명하는지 평가, 여러 검정과 통계치 활용

## 가설검정

모집단 특성에 대한 주장을 가설로 세우고 표본조사를 통해 가설의 채택여부를 판정

**가설검정에 대한 설명으로 옳은 것은?**

1. 연구자에 의해 설정된 가설은 모집단 전체를 근거로 하여 채택여부를 결정짓게 되는데 이때 사용되는 통계량을 검정통계량이라 정의한다.
    
    → 연구자에 의해 설정된 가설은 **표본**을 근거로 하여 채택여부를 결정짓게 되는데 이때 사용되는 통계량을 **검정통계량**이라 정의한다.
    
2. 검정통계량의 표본분포에 따라 채택여부를 결정짓는 일련의 통계적 분석과정을 가설검정이라 하며 일반적으로 몇 단계의 절차를 거쳐 검정이 수행된다.
3. 귀무가설(Null Hypothesis, H_0)은 연구자가 모수에 대해 새로운 통계적 입증을 이루어 내고자 하는 가설이다.
    
    → 귀무가설은 일반적으로 생각하는 가설
    
4. 대립가설(Alternative Hypothesis, H_1)은 현재 통념적으로 믿어지고 있는 모수에 대한 주장 또는 원래의 기준이 되는 가설이다.
    
    → 대립가설은 증명하고자 하는 가설
    

### 기각역

귀무가설을 기각하게 되는 검정통계량의 범위

### 신뢰구간

모수가 포함될 가능성이 있는 구간

모집단의 평균(진짜 평균)이 어느 범위 안에 있을지를 확률적으로 추정하는 방법

구간의 비율은 **신뢰수준**

### **95%** 신뢰구간 구하기

**어느 초등학교 1학년 여자아이들의 혈압자료에서 5명을 랜덤하게 택한 결과가 다음과 같다고 할 때 <102 92 98 88 104> 이를 이용하여 초등학교 여학생 혈압의 대표 값에 대한 95% 신뢰구간을 가장 근사하게 분석한 결과를 고르시오.**

<aside>

표본결과 초등학교 여자아이들의 혈압은 88에서 105 사이에 있다고 할 수 있다.

</aside>

이 표본으로부터 계산된 표본평균과 S는 각각 다음과 같다.

표본평균 = 96.80, S = 6.72

이 표본의 경우 자유도는 5-1=4이다. 표로부터 자유도가 4인 t_0.025경우는 2.78이므로 공식에 대입하면 다음이 성립한다.

목표값 = 96.8 ± 2.78(6.72/2.24) = 96.8 ± 8.32

여기서 2.24는 표본크기 5의 제곱근이다. 따라서 이 표본결과 초등학교 여자아이들의 혈압은 88에서 105 사이에 있다고 할 수 있다.

### 임계치(Critical Value)

주어진 유의수준 α에서 귀무가설의 채택과 기각에 관련된 의사결정을 할 때, 그 기준이 되는 점

### 제 1종 오류

귀무가설이 참일 때 귀무가설을 기각하도록 결정하는 오류

### 제 2종 오류

귀무가설이 거짓인데 귀무가설을 채택할 오류

### 유의수준 α

귀무가설이 참인데도 이것을 기각하게 될 확률

= 1종 오류를 범할 확률의 허용 한계

**통계검정에서 가설을 검정할 시 어느 정도로 검정할 지에 대한 수준으로 표본 추출 시 모수를 포함할 구간의 비율을 일컫는 용어는?**

1. 기각수준
2. 신뢰수준
3. 검정수준
4. 유의수준

**정답 : 2**

신뢰구간은 모수가 포함될 가능성이 있는 구간으로 구간의 비율은 신뢰수준이라고 한다.

**다음은 범주형 분석방법에 대한 설명이다. 옳지 않은 것은?**

1. 빈도분석은 질적 자료를 대상으로 빈도와 비율을 계산할 때 쓰인다.
2. 로지스틱분석은 두 범주형 변수가 서로 상관이 있는지 독립인지를 판단하는 통계적 검정방법이다. → 독립성 검정은 **카이제곱 검정**
로지스틱 분석은 두 개 이상의 독립변수(연속형, 범주형 모두 가능)와 이진 종속변수 간의 관계를 모델링하여 각 범주에 속할 확률을 예측
3. T 검정은 독립변수가 범주형(두개의 집단)이고 종속변수가 연속형인 경우 사용되는 검정 방법으로 두 집단간의 평균 비교 등에 사용된다.
4. 독립변수가 범주형(두개이상 집단)이고 종속변수가 연속형인 경우 사용되는 검정 방법으로 분산분석이 사용된다.

독립성 검정 → 서로 독립적인지

적합도 검정 → 데이터가 특정 이론적 분포를 따르는지

동질성 검정 → 동일한 분포를 가지는지

분포도 검정 → 특정 (정규, 포아송, 이진)분포를 따르는지

## 독립성 검정

두 범주형 변수가 서로 독립적인지를 검정하는 기법

- **사용 예시:**
    - 성별(남/여)과 선호하는 브랜드(A/B/C) 사이에 연관성이 있는지 검정
    - 고객의 나이대(10대/20대/30대)와 구매 여부(구매함/구매 안 함)가 독립적인지 검정
- **검정 방법:** 카이제곱 검정(χ² 검정) 사용
- **핵심 개념:**
    - 귀무가설(H₀): 두 변수는 서로 독립이다.
    - 대립가설(H₁): 두 변수는 서로 독립이 아니다(즉, 관련이 있다).

## 적합도 검정

데이터가 특정 이론적 분포를 따르는지를 검정

- **사용 예시:**
    - 주사위를 100번 던졌을 때, 나온 눈의 분포가 균등한지를 검정
    - 한 가게의 하루 방문자 수가 포아송 분포를 따르는지를 검정
- **검정 방법:** 카이제곱 적합도 검정(χ² 적합도 검정) 사용
- **핵심 개념:**
    - 귀무가설(H₀): 데이터가 특정 분포를 따른다.
    - 대립가설(H₁): 데이터가 특정 분포를 따르지 않는다.

## 동질성 검정

여러 집단에서 동일한 분포를 가지는지를 검정

- **사용 예시:**
    - 지역 A와 지역 B에서 소비자들이 선호하는 브랜드(A/B/C)의 비율이 동일한지 검정
    - 세 개의 회사에서 직원들의 직급별 분포(사원/대리/과장/부장)가 같은지를 검정
- **검정 방법:** 카이제곱 검정(χ² 검정) 사용
- **핵심 개념:**
    - 귀무가설(H₀): 모든 집단에서 분포가 동일하다.
    - 대립가설(H₁): 적어도 하나의 집단에서 분포가 다르다.

## 분포도 검정

데이터가 특정 분포(정규분포, 포아송분포, 이항분포)를 따르는지 검정

- **사용 예시:**
    - 어떤 회사의 월별 판매량 데이터가 정규분포를 따르는지 검정
    - 주사위를 던져 나온 숫자들이 균등분포를 따르는지 검정
- **검정 방법:**
    - Kolmogorov-Smirnov(KS) 검정
    - Shapiro-Wilk 검정 (정규성 검정에 사용)
    - Anderson-Darling 검정

### 잔차

실제 값과 회귀 모델이 예측한 값 간의 차이

### 잔차 진단

- 회귀 모델이 적절한지 평가하는 중요한 과정
- **정규성을 만족, 등분산성을 가지며, 독립적**이면 회귀 분석의 가정이 충족되었음을 의미
- 잔차진단의 유형
    - **정규성 진단**
    
    <aside>
    
    잔차가 **정규분포를 따르는지** 확인하는 과정이다.
    
    - 정규성을 만족하면 **잔차가 평균 0을 중심으로 고르게 분포**하므로, 회귀 모델의 신뢰성이 높아진다.
    - **확인 방법:**
        - **히스토그램**: 잔차의 분포를 보고 정규성을 판단
        - **Q-Q Plot**: 정규분포를 따르는지 시각적으로 분석
        - **정규성 검정(Kolmogorov-Smirnov test, Shapiro-Wilk test 등)**
    </aside>
    
    - **등분산성 진단**
    
    <aside>
    
    잔차의 분산이 일정한지를 확인하는 과정이다.
    
    - 등분산성을 만족하면 회귀 모델이 **모든 값에 대해 일정한 신뢰성을 가짐**
    - 등분산성이 깨지면 **잔차의 크기가 특정 구간에서 커지거나 작아지는 문제 발생** → 예측 신뢰도 감소
    - **확인 방법:**
        - **잔차 vs 예측값 산점도(Residual plot)**
        - **브레쉬-파간 검정(Breusch-Pagan test)**
    </aside>
    
    - **독립성 진단**
    
    <aside>
    
    잔차끼리 **서로 상관되지 않고 독립적인지** 확인하는 과정이다.
    
    - 독립성이 깨지면 **잔차가 특정 패턴을 가지면서 자기상관(autocorrelation) 문제가 발생**
    - 시계열 데이터에서는 특히 중요함 (ex. 시간이 흐를수록 오차가 커지는 경우)
    - **확인 방법:**
        - **더빈-왓슨 검정(Durbin-Watson test)**
    </aside>
    

## 군집분석

집단에 관한 사전정보가 전혀 없는 각 표본에 대하여 그 분류체계를 찾을 때 사용되는 기법

⇒ 판별분석과 달리 집단이 사전에 정의 X

### 비계층적 방법

자료의 산포를 나타내는 여러가지 측도를 이용하여 이들 판정기준을 최적화시키는 방법으로 군집을 나누는 방법

한 번 분리된 개체도 반복적으로 시행하는 과정에서 재분류될 수 있음.

## 군집추출

**군집추출에 대한 설명 중 틀린 것은?**

1. 추출 모집단에 대해 사전지식이 많지 않은 경우 시행하는 방법이다. → **단순 무작위추출방법**
2. 모집단을 차이가 없는 여러 개 군집으로 나누어 군집 단위의 일부 또는 전체에 대한 분석을 시행한다.
3. 모집단에 대한 추출기반을 마련하기가 어려운 경우 사용하면 편리하다.
4. 표본크기가 같은 경우 단순 임의추출에 비해 표본오차가 증대할 가능성이 있다.

## 확률

통계적 현상의 확실함의 정도를 나타내는 척도, 랜덤 시행에서 어떠한 사건이 일어날 정도를 나타내는 사건에 할당된 수들

### 수학적 확률

표본공간 S의 각 근원 사건이 일어날 가능성이 동등할 때, 사건 A에 대하여 n(A)/n(S)

### 통계적 확률

일반적인 자연 현상이나 사회 현상에서 일어날 가능성이 동일한 현상

통계적 확률 시행을 무한 번 반복 → 수학적 확률

**확률에 대한 설명 중 틀린 것은?**

1. 표본공간 S의 각 근원 사건이 일어날 가능성이 동등할 때, 사건 A에 대하여 n(A)/n(S)를 사건A의 수학적 확률이라고 한다.
2. 통계적 확률은 일반적인 자연 현상이나 사회 현상에서 일어날 가능성이 동일한 현상은 드물고 분명하지 않은 경우가 대부분이다.
3. 수학적 확률을 무한 번 시행하면 그 값은 통계적 확률의 값으로 수렴한다. 
→ **통계적 확률 시행을 무한 번 반복시행하면 수학적 확률을 값으로 수렴한다.**
4. 확률은 통계적 현상의 확실함의 정도를 나타내는 척도이며, 랜덤 시행에서 어떠한 사건이 일어날 정도를 나타내는 사건에 할당된 수들을 말한다.

## 변수선택 방법

- 전진 선택법: 큰거부터 하나씩 추가
    - (가) 영 모형에서 시작, 모든 독립변수 중 종속변수와 단순상관계수의 절댓값이 가장 **큰** 변수를 분석모형에 포함시키는 것을 말한다.
    - (나) 부분 F 검정(F-test)을 통해 유의성 검증을 시행, 유의한 경우는 가장 큰 F 통계량을 가지는 모형을 선택하고 유의하지 않은 경우는 변수선택 없이 과정을 중단한다.
    - (다) 한번 추가된 변수는 제거하지 않는 것이 원칙이다 .
- 후진 선택법
    - 모두 있는 상태에서 작은거부터 하나씩 제거
    - 전체모델에서 시작, 모든 독립변수 중 종속변수와 단순상관계수의 절댓값이 가장 **작은** 변수부터 순차적으로 분석모형에서 제외시킨다.
- 단계별 선택법
    - 전진 + 후진
    - 둘 다 한번 추가된 변수에 대해서 제거하지 않는 것이 원칙

## 확률분포

- **확률분포** : 확률변수가 특정 값을 가질 확률을 나타내는 **함수**
    1. ***이산 확률분포*** - 값을 셀 수 있는 분포 (확률질량함수)
        - 이산균등분포 : 모든 곳에서 값 일정
        - 베르누이분포 : 매 시행마다 오직 2가지 결과 뿐
        - 이항분포 : n번의 독립적 베르누의 시행 중 성공할 확률 p를 가지는 분포
        - 기하분포 : 처음 성공할 때까지의 시도횟수를 확률변수로 가지는 분포
        - 다항분포 : 여러 값을 가질 수 있는 확률변수들에 대한 분포
        - 포아송분포 : 단위 공간 내에서 발생할 수 있는 사건의 발생 횟수 표현
            - 관련 문제
                
                **포아송분포를 적용할 수 있는 예가 아닌 것을 고르시오.**
                
                1. 10시부터 11시 사이에 은행지점창구에 도착한 고객의 수
                2. 하루 동안 걸려 오는 전화 수
                3. 원고집필 시 원고지 한 장당 오타의 수
                4. 금융상품 가입 상담건수 10회 중 실제 가입이 이루어진 수
                
                **정답 : 4**
                
                포아송분포 : 단위 시간 안에 어떤 사건이 몇 번 발생할 것인지를 표현하는 이산확률분포이다.
                
        
        ⇒ **베포항항하**
        
    2. ***연속 확률분포*** - 값을 셀 수 없는 분포 (확률밀도함수)
        - 정규분포 - Z검정
        - t분포 : 두 집단의 평균치 차이 비교 - T검정
            - 데이터 개수 **30개 이상**이면 정규성 검정 불필요
            - 이유) 데이터 개수가 많을수록 정규분포랑 유사한 형태가 되기 때문
        - 스튜던트 t분포: 정규분포의 평균 측정 시 사용
            - 종 모양으로서 t=0에 대하여 대칭을 이루는데 t-곡선의 모양을 결정하는 것은 자유도
            - 자유도가 클수록 정규분포에 모양이 수렴
            - 자유도가 1보다 클 때만 스튜던트 t분포에서 기대값 0
        - 카이제곱분포 : 두 집단의 동질성 검정 / 단일 집단 모분산에 대한 검정 - 카이제곱검정
        - F분포 : 두 집단 분산의 동일성 검정 - F검정
        - 지수분포

## 언더샘플링

- 다수 클래스 데이터에서 일부만 사용하는 방법
- 대표 클래스(Majority Class)의 일부만을 선택하고, 소수클래스(Minority Class)는 최대한 많은 데이터를 사용하는 방법

## 오버샘플링

- 소수 클래스 데이터를 증가를 시키는 것
- 소수클래스(Minority Class)의 복사본을 만들어, 대표클래스(Majority Class)의 수만큼 데이터를 만들어 주는 것

## 변수변환

### 로그변환

- 어떤 수치 값을 그대로 사용하지 않고 로그를 취한 값을 사용하는 것
- 로그를 취하면 그 분포가 정규 분포에 가깝게 분포하는 경우가 있다. 이런 분포를 로그정규분포(Log-normal Distribution)를 가진다고 한다.
- 로그변환을 사용하는 데이터 중 대표적인 것은 주식가격의 변동성 분석이다.
- 데이터분포의 형태가 우측으로 치우친 경우 정규분포화를 위해 로그변환을 사용한다

## 이상치와 결측치

### 결측치

필수적인 데이터가 입력되지 않고락

### 이상치

> 이상치가 비 무작위성(Non-Randomly)을 가지고 나타나게(분포하게) 되면 데이터의 정상성(Normality) 감소를 초래하며 이는 데이터 자체의 신뢰성 저하로 연결될 가능성이 있다. 정상성이 높아지면 데이터의 신뢰도가 높아진다.
> 
- 자료처리오류(Data Processing Error)는 복수개의 데이터셋에서 데이터를 추출·조합하여 분석 시, 분석 전의 전처리에서 발생하는 에러를 말한다.
- 비 모수적 이상치를 탐지하는 방법 중에는 산점도그림(Scatter Plot)을 이용한 방법이 있다.
- 의도적 아웃라이어(Intentional Outlier)의 예는 남성의 키를 조사 시 의도적으로 키를 높게 기입하는 경우 등이 있다.

**이상치 발견의 통계적 기법 활용을 설명한 것 중 옳은 것은?**

1. 데이터의 중심을 알기 위해서는 평균(mean), 중앙값(median), 최빈값(mode), 첨도(kurtosis)를 사용할 수 있다.
2. 데이터의 분산도를 알기 위해서는 범위(range), 분산(variance) 왜도(skewness)를 사용할 수 있다.
3. 평균에는 집합 내 모든 데이터 값이 반영되기 때문에, 이상값이 있으면 값이 영향을 받는다.
4. 중앙값은 전체변수의 범위 중에서 가운데값을 사용하므로 이상값이 존재하면 영향을 받는다.
    
    중앙값은 전체변수 범위에서 가운데가 아니라 **관찰된 변수들 중의 가운데값** → 이상값의 영향 ✖️
    

**나이대별 성별과 체중에 대해서 조사를 하고자 한다. 이때 발생 가능한 결측치에 대해서 분류를 다음 아래와 같이 구분하였다. 옳은 것은?**

1. 데이터의 누락 : 비 무작위 결측
→ 완전 무작위 결측
2. 여성은 체중 공개를 꺼림 : 무작위 결측
3. 젊은 여성은 체중공개를 꺼림 : 비 무작위 결측 
→ 무작위 결측
4. 무거운 사람은 체중 공개를 꺼림 : 무작위 결측
→ 비 무작위 측

<aside>
💡

나이대별(X) 성별(Y)과 체중(Z) 분석에 대한 모델링을 가정해 보면

X, Y, Z와 관계없이 Z가 없는 경우 : 데이터의 누락(응답 없음) → 완전 무작위 결측(MCAR)

여성(Y)은 체중공개를 꺼려 하는 경향 : Z가 누락될 가능성이 Y에만 의존→ 무작위 결측(MAR)

젊은(X) 여성(Y)의 경우는 체중공개를 꺼리는 경우가 더 높음 → 무작위 결측(MAR)

무거운(가벼운) 사람들은 체중 공개 가능성이 적음 : Z가 누락될 가능성이 Z값 자체에 관찰되지 않는 값에 달려 있음 → 비 무작위 결측(NMAR)

</aside>

## 차원축소

**차원축소 필요성에 대한 설명으로 틀린 것은?**

1. 데이터를 분석하는 데 있어서 분석시간의 증가(시간복잡도: Time Complexity)와 저장변수 양의 증가(공간복잡도: Space Complexity)를 고려 시 동일한 품질을 나타낼 수 있다면 효율성 측면에서 데이터 종류의 수를 줄여야 한다.
2. 차원이 작은 간단한 분석모델일수록 내부구조 이해가 용이하고 해석이 쉬워진다.
3. 차원의 증가는 분석모델 파라메터의 증가 및 파라메터 간의 복잡한 관계의 증가로 분석결과의 오적합 발생의 가능성이 커진다. 이것은 분석모형의 정확도(신뢰도) 저하를 발생시킬 수 있다.
    
    → 오적합이 아니라 **과적합**
    
4. 작은 차원만으로 안정적인(robust) 결과를 도출해낼 수 있다면 많은 차원을 다루는 것보다 효율적이다.

**정답 : 3**

차원의 증가는 분석모델 파라메터의 증가 및 파라메터 간의 복잡한 관계의 증가로 분석결과의 과적합 발생의 가능성이 커진다. 이것은 분석모형의 정확도(신뢰도) 저하를 발생시킬 수 있다.

## 주성분 분석 **(PCA: Principal Component Analysis)**

<aside>

(가) 분포된 데이터들의 특성을 설명할 수 있는 하나 또는 복수 개의 특징(주성분: Principal Component)을 찾는 것을 의미한다.

(나) 서로 연관성이 있는 고차원공간의 데이터를 선형연관성이 없는 저차원(주성분)으로 변환하는 과정을 거친다(직교변환을 사용).

(다) 기존의 기본변수들을 새로운 변수의 세트로 변환하여 차원을 줄이되 기존 변수들의 분포특성을 최대한 보존하여 이를 통한 분석결과의 신뢰성을 확보한다.

(라) 차원 축소에 폭넓게 사용된다. 어떠한 사전적 분포 가정의 요구가 없다.

(마) 차원의 축소는 본래의 변수들이 서로 상관이 있을 때만 가능하다.

</aside>

## 앙상블 기법

**주어진 자료를 이용하여 여러 가지 분석 예측모형들을 만들고 해당 예측모형들을 결합하여 최종적인 하나의 예측모형을 만드는 기법**

앙상블(Ensemble) 기법은 서로 다른 학습 알고리즘을 경쟁시키는 것이 아닌, 여러 약한 모델을 결합하여 최적화된 모델을 만드는 방식이다.

- 한 개의 Single Leaner에 의한 분석보다는 더 나은 분석성능을 이끌어 낼 수 있다.

### 소프트 보팅

최종 결과물이 나올 확률 값을 다 더해서 최종 결과물에 대한 각각의 확률을 구한 뒤 최종 값을 도출해내는 방법

### 부스팅

가중치를 활용하여 연속적인 Weak learner를 생성하고 이를 통해 강분류기를 만드는 방법

- 강학습기(강분류기)는 Weak Learner로부터 만들어내는 강력한 학습 규칙을 의미한다.
- 약학습기는 무작위 선정이 아닌 성공확률이 높은 즉 오차율이 일정 이하(50% 이하)인 학습 규칙을 말한다.

### 배깅

샘플을 여러 번 뽑아 각 모델을 학습시켜 결과물을 집계(Aggregation)하는 방법

배깅을 활용한 모델 → 랜덤 포레스트(Random Forest)

## 랜덤 포레스트

- 여러 개의 의사결정 나무를 활용하여, 예측 결과를 투표 방식으로 예측 결정한다.
- 랜덤 포레스트는 투표 또는 다수결 방식으로 예측 결정

## 분석모형

### 분석모형 종류

- 예측분석
- 현황진단
- 최적화분석

### 분석 모형 평가 과정

- 분석 알고리즘 적합도 검정

### 분석모형 리모델링

- 데이터 품질 검토
- 분석 알고리즘 개선
- 매개변수 최적화

### 분석모형 설계

- 상향식 접근
    - 특정 영역을 지정, 의사결정 지점으로 진행하는 과정에서 분석기회를 발굴하는 방식
- 하향식 접근
    - 문제탐색, 문제정의, 해결방안탐색, 타당성 평가
- 의사결정나무 기반 접근
- 분석 유스케이스 기반 접근

### 📌 정리

| 접근 방식 | 설명 | 특징 |
| --- | --- | --- |
| **상향식 접근** | 특정 영역에서 시작해 점진적으로 분석을 확장 | 데이터 중심 분석탐색적 데이터 분석과 유사 |
| **하향식 접근** | 목표와 의사결정 지점을 먼저 설정 후 분석 수행 | 비즈니스 목표 중심명확한 방향성 필요 |
| **의사결정나무 기반 접근** | 의사결정 과정에서 데이터를 조건별로 분기하며 분석 | 조건별 최적 경로 선택논리적 분석 가능 |
| **분석 유스케이스 기반 접근** | 기존의 분석 사례를 참고하여 새로운 분석 모형 설계 | 도메인별 최적화실용적인 접근 가능 |

## 의사결정나무

- 대표 알고리즘
    - **CART(Classification and Regression Tree): 불순도 측도로 범주형 또는 이산형일 경우 ( 지니지수 )를, 연속형인 경우 분산의 감소량을 이용한 ( 이진분리 )를 활용**

**의사결정나무의 요소별 설명으로 틀린 것은?**

- 자식마디(child node) : 하나의 마디로부터 분리된 2개 이상의 마디
- 가지(branch) : 하나의 마디로부터 끝 마디까지 연결된 마디
- 부모마디(parent node) : 자식마디의 상위 마디
- 깊이(depth) : 자식마디를 이루는 마디의 개수
    
    → 깊이(depth)는 가지를 이루는 마디의 개수이다.
    

**정답 : 4**

## K-평균 군집

- **비계층적 군집분석은 분석 전에 군집의 수를 정해놓고 군집의 중심으로부터 가까운 순으로 군집에 들어갈 데이터를 정하는 방법을 의미한다.**
- **각각 중심에 모인 데이터를 같은 군집으로 할당하는 대표적인 군집 분석알고리즘**
- K-평균 군집은 기준점에 가까운 곳의 데이터들을 하나의 군집으로 묶는 방법이다.

## SVM

<aside>

SVM(Support Vector Machine) 분석모델은 지도학습 기법으로써 고차원 또는 무한 차원의 공간에서 초평면의 집합을 찾아 이를 이용하여 분류와 회귀를 수행한다. SVM의 핵심적 특징은 기존 분류기가 오류율 최소화에 있다면 여백(마진) 최대화로 일반화 능력의 극대화를 추구하는 점으로 마진이 가장 큰 ( ⓐ )을 분류기(classifier)로 사용할 때 새로운 자료에 대한 오분류가 가장 낮아진다. 또한 경계면과 수직인 법선벡터를 w라고 할 때 마진은 ( ⓑ )로 계산된다.

</aside>

초평면, 2/|W|

## 분석 예측모형

- 미래에 대한 현상을 사전에 분류, 예측하는 것
- 예시: 적조 예측, 날씨 예측, 주가 예측, 범죄/위험 예측, 쇼핑아이템 추천

## 다변량 분산분석

- 두 개 이상의 범주형 독립변수와 다수의 계량적 종속변수 간의 관련성을 동시에 알아볼 때 이용되는 통계적 방법이다.
- 두 개 이상의 계량적 종속변수에 대한 각 집단의 반응치의 분산에 대한 가설을 검증하는데 매우 유용하다.
- 일변량 분산분석의 확장된 형태로 분산을 기준으로 집단간의 통계적 연관성의 가설을 검증하는데 사용된다.

## 로지스틱 회귀분석

분석하고자 하는 대상들이 두 집단으로 나누어진 경우 개별관측치들이 어느 집단으로 분류될 수 있는지를 분석할 때 사용한다.

## 분류분석

- 분석모형에서 종속변수가 없을 시에 사용할 수 있는 알고리즘
- 종속변수: 결과값 변수로 지도학습 기반 분석모형에 해당되는 것

## R

- 객체지향 언어, 풍부한 시각화 라이브러리, 웹브라우저 연동 모듈
- 대용량 메모리 처리 어려움, 보안 기능 취약

## 데이터 시각화

- 다차원 척도법
    - 모든 변수를 비교해서 비슷한 대상을 그래프 상에 가깝게 배치
    - 대상들 간의 거리 또는 유사성 이용 → 원래의 차원보다 낮은 차원의 공간상에 위치시킴 → 대상 간의 유사성 파악 가능

### 시간시각화

- 선그래프
- 영역 차트
- 추세선
- 누적막대그래프
    - 값을 표현하는 하나의 막대에 2개 이상의 변수값으로 구성
    - 시간의 변화에 따른 각각의 변수의 값의 변화 파악 가능
    - 막대그래프에서 굵기는 데이터 값과 상관 없음
    - 막대를 구성하는 각각의 변수는 색이나 패턴 등으로 구별

### 공간시각화

- 버블맵 / 버블 플롯맵
- 카토그램

### 분포시각화

- 트리맵
- 파이차트
- 도넛차트
- 누적 영역 그래프

### 관계시각화

- 버블차트 (지도와 함께 사용될 때 공간시각화 도구로 사용가능)
- 히스토그램
- 누적히스토그램
    - 데이터의 출현 빈도를 오른쪽으로 누적하면서 표시
    - 마지막 막대는 전체 데이터의 총 수를 나타냄
- 산점도
    - 다수의 객체를 군집으로 나누어 그룹 클러스터별 단위로 분석하는 군집분석에서 적용되는 시각화 기법

### 비교시각화

- 히트맵
- 평행좌표계
- 체르노프 페이스
- 스타차트

**분석모델별 활용되는 시각화 기법 설명에서 틀린 것은?**

1. SVM : 비교시각화 기법의 산점도
2. KNN : 관계시각화 기법의 평행좌표계
3. 의사결정나무 : 관계시각화 기법의 트리 다이어그램
4. 회귀모델 : 관계시각화 기법의 히트맵

**정답 : 2**

KNN은 비교시각화 기법의 평행좌표계로써 변수들과의 연관성 및 그룹데이터의 경향성을 파악한다.

### 딥러닝 모델 시각화 방법

- 산포도
- 차원축소
- 노드-링크 다이어그램

→ 딥러닝 모델은 파라미터, **가중치 시각화** 및 **특징 차원감소**를 통해 노드-링크 다이어그램, 산포도, 차원축소 등으로 시각화할 수 있다.

## 군집분석

### 군집타당성지표(Clustering Validity Index)

군집 간 분산과 군집 내 분산으로 군집 간 거리, 군집의 지름, 군집의 분산 등을 고려한다.

## 혼동행렬(오분류표) = 오차행렬

- 예측과 실제가 같으면 True, 예측이 True → Positive

|  | 실제 True | 실제 False |
| --- | --- | --- |
| 예측 True | True Positive | False Positive |
| 예측 False | False Negative | True Negative |

**암에 대해 양성과 음성 데이터를 분류할 시 예측 경우의 수로 적합한 설명을 고른다면?**

1. 양성인데 음성으로 검출되면 False Negative이다.
2. 음성인데 양성으로 검출되면 True Negative이다. → 실제 음성인데 양성으로 예측 False Positive
3. 음성인데 음성으로 검출되면 False Negative이다. → 예측과 실제가 같으면 True이어야 함
4. 양성인데 양성으로 검출되면 False Positive이다. → 예측과 실제가 같으면 True이어야 함

**정답 : 1**

False Negative는 실제 True(양성)인 정답을 False(음성)라고 예측하는 의미이다.

## 범주형 자료분석

분석에 사용되는 변수들이 변주 혹은 이산형인 경우 사용

| 독립변수 | 종속변수 | 분석 방법 |
| --- | --- | --- |
| 범주형 | 범주형 | 분할표 분석
카이제곱 검정 |
|  | 연속형 | T 검정
분산 분석 |
| 연속형 | 범주형 | 로지스틱 회귀 |

**적합도 검정 기법 종류와 맞지 않는 기법은?**

1. 정규성 검정
2. 카이제곱 검정 → 독립, 종속 변수 둘다 범주형 변수
3. T검정 → 종속 변수는 연속형
4. 콜모고로프 스미르노프 검정

**정답 : 3**

적합도 검정이란 범주형 변수에 관한 분석 방법으로 연속형 변수에 활용되는 검정기법인 T검정은 해당되지 않는다.

## 적합도 검정

- 범주형 변수가 특정 분포(정규분포, 균등분포)를 따르는지 검정하는 것
- **독립변수가 없어도 수행 가능**하며, 종속변수(혹은 분석 변수)가 특정 분포를 따르는지를 검정하는 데 사용됨.
- 범주별 빈도가 기대값과 얼마나 차이가 있는지 평가
- 예시
    - **카이제곱 적합도 검정(Chi-Square Goodness-of-Fit Test) → 범주형 데이터**
    - **콜모고로프-스미르노프 검정(Kolmogorov-Smirnov Test, K-S Test) (연속형에도 사용됨)**

### ✅ **대표적인 정규성 검정 방법**

| 검정 방법 | 특징 |
| --- | --- |
| **샤피로-윌크 검정 (Shapiro-Wilk Test)** | 작은 샘플에서도 강력한 정규성 검정 방법 (n ≤ 50 추천) |
| **콜모고로프-스미르노프 검정 (K-S Test)** | 정규분포뿐만 아니라 여러 분포와의 적합도를 검정 가능 |
| **앤더슨-달링 검정 (Anderson-Darling Test)** | 샤피로-윌크보다 대형 샘플에 적합 |
| **자비스-베라 검정 (Jarque-Bera Test)** | 왜도(Skewness)와 첨도(Kurtosis)를 이용하여 정규성 검정 |

📌 **모든 정규성 검정은 "데이터가 정규분포를 따르는지 확인"하는 것이므로 적합도 검정의 일종**

### ✅ **정규성 검정과 카이제곱 적합도 검정의 차이**

| 검정 방법 | 데이터 유형 | 주로 사용되는 경우 |
| --- | --- | --- |
| **카이제곱 적합도 검정** | 범주형 데이터 | 범주형 변수가 기대 분포(예: 균등분포)를 따르는지 검정 |
| **정규성 검정 (샤피로-윌크, K-S, 앤더슨-달링 등)** | 연속형 데이터 | 데이터가 정규분포를 따르는지 검정 |

👉 즉, **범주형 데이터의 분포 적합도를 검정할 때는 카이제곱 검정을 사용하고, 연속형 데이터의 정규성을 확인할 때는 정규성 검정을 사용**

## 신경망 학습

<aside>

(가) 기계학습과 인지과학에서 생물학의 신경망(동물의 중추신경계중 특히 뇌)에서 영감을 얻은 통계학적 학습 알고리즘이다.

(나) 시냅스의 결합으로 네트워크를 형성한 인공 뉴런(노드)이 학습을 통해 시냅스의 결합 세기를 변화시켜, 문제 해결 능력을 가지는 모델 전반을 가리킨다.

(다) 트레이닝 셋에만 최적화되어 실제 테스트와 예상결과의 괴리가 발생하는 단점이 있다.

(라) 최초 시작점의 선택에 따라 수렴, 발산, 진동 등 다양한 형태로 결과가 바뀌는 단점이 있다.

</aside>

### 신경망 알고리즘 학습 프로세스

미니배치 → 가중치 매개변수 기울기 산출 → 매개변수 갱신

### 뉴런의 연결방법

- 층간 연결: 서로 다른 층에 존재하는 뉴런과 연결
- 층내 연결: 동일층 내의 뉴런과의 연결
- 순환 연결: 어떠한 뉴런의 출력이 자기 자신에게 입력되는 연결

### 데이터 분할

**학습 데이터 (Training Set)** → 모델이 패턴을 학습하는 데이터

**검증 데이터 (Validation Set)** → 모델 성능을 조정하고 최적화하는 데이터

**테스트 데이터 (Test Set)**→ 모델의 최종 성능을 평가하는 데이터

### **1️⃣ 학습 데이터셋 (Training Set)**

- **역할:** 모델이 패턴을 학습하는 데 사용
- **특징:** 데이터의 대부분(보통 60~80%)을 차지하며, 모델이 가중치(weight)와 편향(bias)을 조정하는 데 사용
- **예시:** 학생이 교과서를 읽으며 공부하는 과정

---

### **2️⃣ 검증 데이터셋 (Validation Set)**

- **역할:** 학습 중에 **모델의 성능을 확인하고 하이퍼파라미터를 조정하는 용도**
- **특징:** 학습 과정에서 **과적합 여부를 판단하고 최적의 모델을 찾기 위해 사용**
- **보통 10~20%를 사용하며, 교차검증(K-Fold)과 함께 사용될 수도 있음**
- **예시:** 학생이 모의고사를 풀면서 현재 실력을 점검하는 과정

---

### **3️⃣ 테스트 데이터셋 (Test Set)**

- **역할:** 모델이 학습과 검증에 사용되지 않은 새로운 데이터로 **모델의 최종 성능을 평가**
- **특징:** 모델이 완전히 학습된 후, 일반화 성능을 측정하기 위해 사용 (보통 10~20%)
- **예시:** 학생이 실제 시험을 치르는 과정

## 신경망학습 구성요소

| 구성 요소 | 설명 |
| --- | --- |
| **활성화 함수** | 입력 신호를 출력 신호로 변환 |
| **손실 함수** | 모델 예측과 실제값 차이를 측정 |
| **옵티마이저** | 손실을 최소화하는 방향으로 가중치 조정 |
| **역전파 알고리즘** | 오차를 역으로 전파하여 가중치 업데이트 |
| **신경망 구조** | 입력층, 은닉층, 출력층으로 구성 |

### 1. 활성화 함수

- 입력신호의 총합을 출력신호로 변환하는 역할을 하는 함수
- **비선형성**을 추가하여 신경마이 복잡한 패턴을 학습할 수 있도록 함.
- 종류
    - **시그모이드 함수 (Sigmoid Function)**
    
    <aside>
    
    - 값 범위: (0, 1)
    
    $$
    f(x) = \frac{1}{1+e^{-x}}
    $$
    
    - 장점: 확률 값으로 해석 가능 (출력이 0~1 사이)
    - 단점: **기울기 소실 문제** (Gradient Vanishing Problem) 발생
    </aside>
    
    - **ReLU**
    
    <aside>
    
    - 값 범위: (0, ∞)
    
    $$
    f(x)=max⁡(0,x)
    $$
    
    - 장점: 기울기 소실 문제 해결, 연산량 적음
    - 단점: **죽은 ReLU 문제** (출력이 0이 되면 뉴런이 학습되지 않음)
    </aside>
    
    - **Softmax 함수** (출력층에서 사용)
    
    <aside>
    
    - 다중 클래스 분류 문제에서 확률 값 출력
    
    $$
    \sigma(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
    $$
    
    - 각 클래스의 확률을 정규화하여 확률 분포로 변환
    </aside>
    

### 2. 손실 함수 (Loss Function)

- 모델의 예측값과 실제값 사이의 차이를 측정하는 함수
- 최적의 모델을 학습하기 위해 **손실을 최소화**하는 방향으로 가중치를 조정
- **손실 함수의 종류**
1. **회귀 문제 (연속형 출력값)**
    - **평균제곱오차 (MSE, Mean Squared Error)**
    
    $$
    MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
    $$
    
    - **평균절대오차 (MAE, Mean Absolute Error)**
        
        $$
        MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
        $$
        
2. **분류 문제 (범주형 출력값)**
    - **교차 엔트로피 손실 (Cross-Entropy Loss)**
        
        $$
        L = -\sum y_i \log(\hat{y}_i)
        $$
        
    - **이진 교차 엔트로피 (Binary Cross-Entropy, BCE)** (이진 분류)
    - **다중 클래스 교차 엔트로피 (Categorical Cross-Entropy, CCE)** (다중 분류)

### **3. 옵티마이저 (Optimizer)**

- 신경망이 최적의 가중치를 찾도록 학습을 조정하는 알고리즘
- **손실 함수를 최소화**하기 위해 사용
- 일반적으로 **확률적 경사 하강법 (SGD, Stochastic Gradient Descent) 기반**
- **주요 옵티마이저 종류**
1. **확률적 경사 하강법 (SGD, Stochastic Gradient Descent)**
    - 한 개 또는 일부 데이터 샘플을 이용해 가중치를 조정
    - 계산량이 적어 빠르지만, 최적점에 도달하기 어려울 수 있음
2. **모멘텀 (Momentum)**
    - SGD에 속도를 더해 진동을 줄이고 빠르게 최적화하도록 개선
3. **Adam (Adaptive Moment Estimation)**
    - **가장 많이 사용되는 옵티마이저**
    - 학습 속도를 자동으로 조절하여 효율적으로 최적화
4. **RMSprop**
    - 학습률을 자동 조정하여 수렴 속도를 높임

---

### **4. 역전파 알고리즘 (Backpropagation)**

- 신경망의 가중치를 업데이트하는 핵심 알고리즘
- **오차를 출력층에서 입력층으로 역방향으로 전파**하며 가중치를 조정
- **손실 함수의 변화율(미분값)을 이용해 최적의 가중치를 찾음**

### **역전파 과정**

1. **순전파 (Forward Propagation)**
    - 입력층 → 은닉층 → 출력층 방향으로 신호 전달
    - 최종 출력값을 계산
2. **손실 계산 (Loss Calculation)**
    - 예측값과 실제값의 차이를 손실 함수로 계산
3. **역전파 (Backward Propagation)**
    - **오차를 역으로 전파하여 가중치 업데이트**
    - 체인 룰(chain rule)을 이용해 미분 계산
4. **가중치 업데이트 (Weight Update)**
    - 옵티마이저를 이용해 가중치를 수정
    
    $$
    w= w - \alpha \frac{\partial L}{\partial w}
    $$
    

---

### **5. 신경망 구조**

1. **입력층 (Input Layer)**
    - 데이터를 받아들이는 층
    - 뉴런의 개수는 입력 변수(특징) 수와 동일
2. **은닉층 (Hidden Layer)**
    - 입력을 처리하고 특징을 추출하는 층
    - 활성화 함수를 사용하여 비선형 변환 수행
3. **출력층 (Output Layer)**
    - 최종 결과를 출력하는 층
    - 분류 문제 → **Softmax**
    - 회귀 문제 → **선형 함수** 사용

## **GAN(Generative Adversarial Network)**

<aside>

GAN은 2014년 NIPS에서 Ian Goodfellow가 발표한 회귀생성 모델로서 ( ⓐ )을/를 담당하는 모델(판별자 D)과 회귀( ⓑ )을/를 담당하는 두 개의 모델(생성자 G)로 구성되어 있다. 생성자 G와 판별자 D가 서로의 성능을 개선해 적대적으로 경쟁해 나가는 모델로 적대적 학습에서는 ( ⓐ ) 모델을 먼저 학습시킨 후, ( ⓑ ) 모델을 학습시키는 과정을 서로 주고받으면서 반복한다.

</aside>

ⓐ분류, ⓑ생성

### GAN의 구성 요소

1. **판별자(Discriminator, D)**
- 실제 데이터와 생성된 데이터를 **구분(분류, Classification)** 하는 역할
- 진짜(real) 데이터와 가짜(fake) 데이터를 입력받아 "진짜/가짜"를 판별하는 **이진 분류(binary classification)** 문제를 해결
- **분류(Classification)를 담당하는 모델**
1. **생성자(Generator, G)**
- 무작위 노이즈를 입력으로 받아 새로운 데이터를 생성하는 역할
- 데이터의 분포를 학습하여 **실제와 유사한 가짜 데이터를 생성**하는 것이 목표
- **생성(Generation)을 담당하는 모델**

## 초매개변수

- 최적화 기법: 미니배치 크기, 훈련반복 횟수, 은닉층 개수조정

## 과대적합(Overfitting) 방지 기법

✅ **1. 드롭아웃 (Dropout) → 과대적합 방지**

- 신경망의 일부 뉴런을 랜덤하게 비활성화(즉, 0으로 설정)하여 특정 뉴런에 의존하는 것을 방지
- 학습 시마다 다른 부분 네트워크를 학습하게 만들어 **과적합을 줄이는 효과**가 있다.

✅ **2. L2 규제 (L2 Regularization) → 과대적합 방지**

- 가중치가 너무 커지는 것을 방지하여 **모델이 복잡해지는 것 방지**
- 일반적으로 **릿지 회귀(Ridge Regression)** 에 사용되며, 가중치의 제곱합을 패널티로 추가하여 가중치를 작게 유지한다.

✅ **3. L1 규제 (L1 Regularization) → 과대적합 방지**

- 불필요한 가중치를 0으로 만들어 **특정 변수를 자동으로 제거하는 효과(Lasso Regression)**
- 차원의 축소 효과가 있어 **모델의 복잡도를 낮추고 과대적합을 방지**

🚫 **4. 매개변수 최적화 (Parameter Optimization) → 과대적합 방지 기법이 아님**

- 매개변수 최적화는 손실 함수의 값을 최소화하도록 가중치와 편향을 업데이트하는 과정이다.
- 즉, 모델이 **훈련 데이터에 더 잘 맞도록 조정하는 과정**이므로 **과대적합을 방지하는 기법이 아니다**.
- 과대적합을 방지하려면 **정규화(규제), 데이터 증강, 드롭아웃 등 추가적인 조치가 필요**하다.

## 연관분석모델

- 연관규칙을 생성하는  Aporior 알고리즘을 주로 사용
- 두 개 또는 그 이상의 품목들 사이의 특정 요소로 해석 → **상호 관련성**

**분석모델별 결과해석 설명 중 틀린 것은?**

1. 딥러닝 모델 해석에 사용하는 오차율은 절대오차가 활용된다. → **상대오차나 평균 제곱근 편차**
2. 회귀분석모델의 잔차에는 패턴이나 추세가 있어서는 안된다.
3. 군집분석모델은 연속형 변수의 경우 평균 또는 중앙값을 계산한다.
4. 연관분석모델은 지지도, 신뢰도 및 향상도가 높은 규칙들을 찾되 최소 기준점을 적용한다.

## 교차검증

- 데스트 검증데이터가 하나로 고정되지 않게 하며 데이터 모든 부분을 사용하여 모델을 검증
- 훈련 데이터셋의 일부를 훈련으로 이용하여 나머지를 검증으로 활용
- 반복 횟수 증가에 따른 모델 훈련과 평가/검증시간이 오래 걸릴 수 있다.

### **홀드아웃(Holdout) 검증**

- 데이터를 **훈련 세트(train set)와 검증 세트(validation set)로 한 번만 나누어 평가**
- 단순하고 빠르지만, **데이터가 적을 경우 훈련에 활용되지 않은 데이터가 많아 모델 성능이 저하될 가능성** 있음

### K-fold 교차검증 기법 (K-Fold Cross Validation)

- 전체 데이터셋을 K개의 서브넷으로 나누어 K번의 평가를 실행하는데 테스트 셋을 중복없이 병행 진행하여 평균을 내어 최종적 모델의 성능 평가
- **데이터를 K개의 서브셋으로 나눈 후, K번의 반복 학습을 수행**
- 매 반복마다 **한 개의 서브셋을 검증 데이터로, 나머지를 훈련 데이터로 사용**
- 모든 데이터가 한 번씩 검증 데이터로 사용되므로 일반화 성능이 높아짐

### **LOOCV(Leave-One-Out Cross Validation, 한 개씩 제외 교차검증)**

- 데이터셋의 **각 샘플을 테스트 데이터로 사용하고, 나머지를 훈련 데이터로 사용하는 방식**
- **K-Fold의 극단적인 형태 (K=N, 즉 데이터 개수만큼 Fold를 설정)**
- 데이터가 적을 때 유용하지만, **데이터 개수가 많을 경우 연산량이 커서 비효율적**

## 인포그래픽

⇒ 설명이 목적

Information(정보) + Graphic(그래픽)

**복잡한 정보를 쉽게 전달하기 위해 시각적으로 표현한 그래픽**

## 시각적 분석

⇒ 데이터 분석이 목적

<aside>

시각적 분석은 상호작용이 가능한 시각적 인터페이스를 사용해 데이터의 분석적 추론을 진행하는 과정을 말한다. 문제의 크기가 크거나 복잡한 영역에서 시각적으로 다양한 측면에 따라 사람과 컴퓨터가 상호작용하면서 분석을 진행한다.

데이터 시각화의 하위 개념으로 상호작용이 가능한 시각적 인터페이스를 사용해 데이터의 특징을 분석해 나가는 과정

</aside>

### **📊 인포그래픽 vs 시각적 분석 차이점**

| 구분 | 인포그래픽 | 시각적 분석 |
| --- | --- | --- |
| **목적** | 정보를 쉽고 빠르게 전달 | 데이터를 분석하고 인사이트 도출 |
| **특징** | 정적인 이미지, 설명 위주 | 상호작용이 가능한 시각적 인터페이스 |
| **사용 사례** | 뉴스, 보고서, 마케팅 자료 | 데이터 분석, 빅데이터 탐색 |
| **예시** | 포스터, 카드 뉴스, 데이터 요약 | 대시보드, 데이터 시각화 툴 (Tableau 등) |
