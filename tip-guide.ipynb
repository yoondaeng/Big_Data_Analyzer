{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "760bd245",
   "metadata": {
    "papermill": {
     "duration": 0.015313,
     "end_time": "2025-06-19T07:24:47.999801",
     "exception": false,
     "start_time": "2025-06-19T07:24:47.984488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Tips] 빅데이터 분석기사 실기 \n",
    "1. 코드 힌트(자동완성) 없이 이름 찾는 방법\n",
    "2. 함수, 속성 활용법 찾기(공식문서)\n",
    "3. 당황하지 말고, 공식문서 통으로 만들기\n",
    "\n",
    "- 영상: https://youtu.be/2Nf6yAgnZTY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfc60e",
   "metadata": {
    "papermill": {
     "duration": 0.012669,
     "end_time": "2025-06-19T07:24:48.026057",
     "exception": false,
     "start_time": "2025-06-19T07:24:48.013388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Q. 코드 힌트(자동완성)가 없어요! 다 외워야 할까요? 난 못외워...**\n",
    "- 그럼 어떻게??\n",
    "- 판다스와 사이킷런을 예시로 함수, 속성 등의 이름을 모를 때 찾는법을 알아봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb811638",
   "metadata": {
    "papermill": {
     "duration": 0.015184,
     "end_time": "2025-06-19T07:24:48.054455",
     "exception": false,
     "start_time": "2025-06-19T07:24:48.039271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- 아래 링크(시험환경)에서 반드시 연습 해보세요 :)\n",
    "- https://dataq.goorm.io/exam/116674/%EC%B2%B4%ED%97%98%ED%95%98%EA%B8%B0/quiz/3\n",
    "- 시험장에서 필요할 때마다 실행할 순 없으니 아래 코드를 하나씩 실행한 후 메모장에 옮겨 두시기 바랍니다. (메모장 검색기능 활용)\n",
    "- 아래 코드는 어느정도 할 줄 아는 상황에서 철자가 틀렸거나 함수나 속성이 기억이 나지 않을때만 유용합니다.\n",
    "- 입문이라면 먼저 https://www.kaggle.com/agileteam/bigdatacertificationkr 에서 예시문제(Tasks)를 풀어보세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcb808",
   "metadata": {
    "papermill": {
     "duration": 0.013518,
     "end_time": "2025-06-19T07:24:48.081159",
     "exception": false,
     "start_time": "2025-06-19T07:24:48.067641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f274bb3a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:48.118773Z",
     "iopub.status.busy": "2025-06-19T07:24:48.118067Z",
     "iopub.status.idle": "2025-06-19T07:24:48.123077Z",
     "shell.execute_reply": "2025-06-19T07:24:48.123632Z",
     "shell.execute_reply.started": "2025-06-19T07:05:36.044240Z"
    },
    "papermill": {
     "duration": 0.029575,
     "end_time": "2025-06-19T07:24:48.123952",
     "exception": false,
     "start_time": "2025-06-19T07:24:48.094377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BooleanDtype', 'Categorical', 'CategoricalDtype', 'CategoricalIndex', 'DataFrame', 'DateOffset', 'DatetimeIndex', 'DatetimeTZDtype', 'ExcelFile', 'ExcelWriter', 'Flags', 'Float32Dtype', 'Float64Dtype', 'Float64Index', 'Grouper', 'HDFStore', 'Index', 'IndexSlice', 'Int16Dtype', 'Int32Dtype', 'Int64Dtype', 'Int64Index', 'Int8Dtype', 'Interval', 'IntervalDtype', 'IntervalIndex', 'MultiIndex', 'NA', 'NaT', 'NamedAgg', 'Period', 'PeriodDtype', 'PeriodIndex', 'RangeIndex', 'Series', 'SparseDtype', 'StringDtype', 'Timedelta', 'TimedeltaIndex', 'Timestamp', 'UInt16Dtype', 'UInt32Dtype', 'UInt64Dtype', 'UInt64Index', 'UInt8Dtype', '__builtins__', '__cached__', '__doc__', '__docformat__', '__file__', '__getattr__', '__git_version__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_config', '_hashtable', '_is_numpy_dev', '_lib', '_libs', '_np_version_under1p18', '_testing', '_tslib', '_typing', '_version', 'api', 'array', 'arrays', 'bdate_range', 'compat', 'concat', 'core', 'crosstab', 'cut', 'date_range', 'describe_option', 'errors', 'eval', 'factorize', 'get_dummies', 'get_option', 'infer_freq', 'interval_range', 'io', 'isna', 'isnull', 'json_normalize', 'lreshape', 'melt', 'merge', 'merge_asof', 'merge_ordered', 'notna', 'notnull', 'offsets', 'option_context', 'options', 'pandas', 'period_range', 'pivot', 'pivot_table', 'plotting', 'qcut', 'read_clipboard', 'read_csv', 'read_excel', 'read_feather', 'read_fwf', 'read_gbq', 'read_hdf', 'read_html', 'read_json', 'read_orc', 'read_parquet', 'read_pickle', 'read_sas', 'read_spss', 'read_sql', 'read_sql_query', 'read_sql_table', 'read_stata', 'read_table', 'read_xml', 'reset_option', 'set_eng_float_format', 'set_option', 'show_versions', 'test', 'testing', 'timedelta_range', 'to_datetime', 'to_numeric', 'to_pickle', 'to_timedelta', 'tseries', 'unique', 'util', 'value_counts', 'wide_to_long']\n"
     ]
    }
   ],
   "source": [
    "#dir을 통해 사용 가능한 함수 확인\n",
    "import pandas as pd\n",
    "print(dir(pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28530e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:48.157559Z",
     "iopub.status.busy": "2025-06-19T07:24:48.156695Z",
     "iopub.status.idle": "2025-06-19T07:24:48.160095Z",
     "shell.execute_reply": "2025-06-19T07:24:48.160640Z",
     "shell.execute_reply.started": "2025-06-19T07:20:49.744630Z"
    },
    "papermill": {
     "duration": 0.022984,
     "end_time": "2025-06-19T07:24:48.160816",
     "exception": false,
     "start_time": "2025-06-19T07:24:48.137832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', '_AXIS_LEN', '_AXIS_NAMES', '_AXIS_NUMBERS', '_AXIS_ORDERS', '_AXIS_REVERSED', '_AXIS_TO_AXIS_NUMBER', '_HANDLED_TYPES', '__abs__', '__add__', '__and__', '__annotations__', '__array__', '__array_priority__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__divmod__', '__doc__', '__eq__', '__finalize__', '__floordiv__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__imod__', '__imul__', '__init__', '__init_subclass__', '__invert__', '__ior__', '__ipow__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__round__', '__rpow__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_accessors', '_accum_func', '_add_numeric_operations', '_agg_by_level', '_agg_examples_doc', '_agg_summary_and_see_also_doc', '_align_frame', '_align_series', '_arith_method', '_as_manager', '_box_col_values', '_can_fast_transpose', '_check_inplace_and_allows_duplicate_labels', '_check_inplace_setting', '_check_is_chained_assignment_possible', '_check_label_or_level_ambiguity', '_check_setitem_copy', '_clear_item_cache', '_clip_with_one_bound', '_clip_with_scalar', '_cmp_method', '_combine_frame', '_consolidate', '_consolidate_inplace', '_construct_axes_dict', '_construct_axes_from_arguments', '_construct_result', '_constructor', '_constructor_sliced', '_convert', '_count_level', '_data', '_dir_additions', '_dir_deletions', '_dispatch_frame_op', '_drop_axis', '_drop_labels_or_levels', '_ensure_valid_index', '_find_valid_index', '_from_arrays', '_from_mgr', '_get_agg_axis', '_get_axis', '_get_axis_name', '_get_axis_number', '_get_axis_resolvers', '_get_block_manager_axis', '_get_bool_data', '_get_cleaned_column_resolvers', '_get_column_array', '_get_index_resolvers', '_get_item_cache', '_get_label_or_level_values', '_get_numeric_data', '_get_value', '_getitem_bool_array', '_getitem_multilevel', '_gotitem', '_hidden_attrs', '_indexed_same', '_info_axis', '_info_axis_name', '_info_axis_number', '_info_repr', '_init_mgr', '_inplace_method', '_internal_names', '_internal_names_set', '_is_copy', '_is_homogeneous_type', '_is_label_or_level_reference', '_is_label_reference', '_is_level_reference', '_is_mixed_type', '_is_view', '_iset_item', '_iset_item_mgr', '_iset_not_inplace', '_iter_column_arrays', '_ixs', '_join_compat', '_logical_func', '_logical_method', '_maybe_cache_changed', '_maybe_update_cacher', '_metadata', '_min_count_stat_function', '_needs_reindex_multi', '_protect_consolidate', '_reduce', '_reindex_axes', '_reindex_columns', '_reindex_index', '_reindex_multi', '_reindex_with_indexers', '_replace_columnwise', '_repr_data_resource_', '_repr_fits_horizontal_', '_repr_fits_vertical_', '_repr_html_', '_repr_latex_', '_reset_cache', '_reset_cacher', '_sanitize_column', '_series', '_set_axis', '_set_axis_name', '_set_axis_nocheck', '_set_is_copy', '_set_item', '_set_item_frame_value', '_set_item_mgr', '_set_value', '_setitem_array', '_setitem_frame', '_setitem_slice', '_slice', '_stat_axis', '_stat_axis_name', '_stat_axis_number', '_stat_function', '_stat_function_ddof', '_take_with_is_copy', '_to_dict_of_blocks', '_typ', '_update_inplace', '_validate_dtype', '_values', '_where', 'abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'append', 'apply', 'applymap', 'asfreq', 'asof', 'assign', 'astype', 'at', 'at_time', 'attrs', 'axes', 'backfill', 'between_time', 'bfill', 'bool', 'boxplot', 'clip', 'columns', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'dtypes', 'duplicated', 'empty', 'eq', 'equals', 'eval', 'ewm', 'expanding', 'explode', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'flags', 'floordiv', 'from_dict', 'from_records', 'ge', 'get', 'groupby', 'gt', 'head', 'hist', 'iat', 'idxmax', 'idxmin', 'iloc', 'index', 'infer_objects', 'info', 'insert', 'interpolate', 'isin', 'isna', 'isnull', 'items', 'iteritems', 'iterrows', 'itertuples', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'loc', 'lookup', 'lt', 'mad', 'mask', 'max', 'mean', 'median', 'melt', 'memory_usage', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ndim', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'query', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select_dtypes', 'sem', 'set_axis', 'set_flags', 'set_index', 'shape', 'shift', 'size', 'skew', 'slice_shift', 'sort_index', 'sort_values', 'sparse', 'squeeze', 'stack', 'std', 'style', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf', 'to_html', 'to_json', 'to_latex', 'to_markdown', 'to_numpy', 'to_parquet', 'to_period', 'to_pickle', 'to_records', 'to_sql', 'to_stata', 'to_string', 'to_timestamp', 'to_xarray', 'to_xml', 'transform', 'transpose', 'truediv', 'truncate', 'tshift', 'tz_convert', 'tz_localize', 'unstack', 'update', 'value_counts', 'values', 'var', 'where', 'xs']\n"
     ]
    }
   ],
   "source": [
    "#데이터 프레임에서 할 수 있는 것들은?\n",
    "print(dir(pd.DataFrame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f47844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:48.196133Z",
     "iopub.status.busy": "2025-06-19T07:24:48.195266Z",
     "iopub.status.idle": "2025-06-19T07:24:48.198987Z",
     "shell.execute_reply": "2025-06-19T07:24:48.199747Z",
     "shell.execute_reply.started": "2025-06-19T07:22:04.922402Z"
    },
    "papermill": {
     "duration": 0.025076,
     "end_time": "2025-06-19T07:24:48.199933",
     "exception": false,
     "start_time": "2025-06-19T07:24:48.174857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function drop in module pandas.core.frame:\n",
      "\n",
      "drop(self, labels=None, axis: 'Axis' = 0, index=None, columns=None, level: 'Level | None' = None, inplace: 'bool' = False, errors: 'str' = 'raise')\n",
      "    Drop specified labels from rows or columns.\n",
      "    \n",
      "    Remove rows or columns by specifying label names and corresponding\n",
      "    axis, or by specifying directly index or column names. When using a\n",
      "    multi-index, labels on different levels can be removed by specifying\n",
      "    the level. See the `user guide <advanced.shown_levels>`\n",
      "    for more information about the now unused levels.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    labels : single label or list-like\n",
      "        Index or column labels to drop.\n",
      "    axis : {0 or 'index', 1 or 'columns'}, default 0\n",
      "        Whether to drop labels from the index (0 or 'index') or\n",
      "        columns (1 or 'columns').\n",
      "    index : single label or list-like\n",
      "        Alternative to specifying axis (``labels, axis=0``\n",
      "        is equivalent to ``index=labels``).\n",
      "    columns : single label or list-like\n",
      "        Alternative to specifying axis (``labels, axis=1``\n",
      "        is equivalent to ``columns=labels``).\n",
      "    level : int or level name, optional\n",
      "        For MultiIndex, level from which the labels will be removed.\n",
      "    inplace : bool, default False\n",
      "        If False, return a copy. Otherwise, do operation\n",
      "        inplace and return None.\n",
      "    errors : {'ignore', 'raise'}, default 'raise'\n",
      "        If 'ignore', suppress error and only existing labels are\n",
      "        dropped.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or None\n",
      "        DataFrame without the removed index or column labels or\n",
      "        None if ``inplace=True``.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    KeyError\n",
      "        If any of the labels is not found in the selected axis.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.loc : Label-location based indexer for selection by label.\n",
      "    DataFrame.dropna : Return DataFrame with labels on given axis omitted\n",
      "        where (all or any) data are missing.\n",
      "    DataFrame.drop_duplicates : Return DataFrame with duplicate rows\n",
      "        removed, optionally only considering certain columns.\n",
      "    Series.drop : Return Series with specified index labels removed.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = pd.DataFrame(np.arange(12).reshape(3, 4),\n",
      "    ...                   columns=['A', 'B', 'C', 'D'])\n",
      "    >>> df\n",
      "       A  B   C   D\n",
      "    0  0  1   2   3\n",
      "    1  4  5   6   7\n",
      "    2  8  9  10  11\n",
      "    \n",
      "    Drop columns\n",
      "    \n",
      "    >>> df.drop(['B', 'C'], axis=1)\n",
      "       A   D\n",
      "    0  0   3\n",
      "    1  4   7\n",
      "    2  8  11\n",
      "    \n",
      "    >>> df.drop(columns=['B', 'C'])\n",
      "       A   D\n",
      "    0  0   3\n",
      "    1  4   7\n",
      "    2  8  11\n",
      "    \n",
      "    Drop a row by index\n",
      "    \n",
      "    >>> df.drop([0, 1])\n",
      "       A  B   C   D\n",
      "    2  8  9  10  11\n",
      "    \n",
      "    Drop columns and/or rows of MultiIndex DataFrame\n",
      "    \n",
      "    >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'],\n",
      "    ...                              ['speed', 'weight', 'length']],\n",
      "    ...                      codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
      "    ...                             [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
      "    >>> df = pd.DataFrame(index=midx, columns=['big', 'small'],\n",
      "    ...                   data=[[45, 30], [200, 100], [1.5, 1], [30, 20],\n",
      "    ...                         [250, 150], [1.5, 0.8], [320, 250],\n",
      "    ...                         [1, 0.8], [0.3, 0.2]])\n",
      "    >>> df\n",
      "                    big     small\n",
      "    lama    speed   45.0    30.0\n",
      "            weight  200.0   100.0\n",
      "            length  1.5     1.0\n",
      "    cow     speed   30.0    20.0\n",
      "            weight  250.0   150.0\n",
      "            length  1.5     0.8\n",
      "    falcon  speed   320.0   250.0\n",
      "            weight  1.0     0.8\n",
      "            length  0.3     0.2\n",
      "    \n",
      "    >>> df.drop(index='cow', columns='small')\n",
      "                    big\n",
      "    lama    speed   45.0\n",
      "            weight  200.0\n",
      "            length  1.5\n",
      "    falcon  speed   320.0\n",
      "            weight  1.0\n",
      "            length  0.3\n",
      "    \n",
      "    >>> df.drop(index='length', level=1)\n",
      "                    big     small\n",
      "    lama    speed   45.0    30.0\n",
      "            weight  200.0   100.0\n",
      "    cow     speed   30.0    20.0\n",
      "            weight  250.0   150.0\n",
      "    falcon  speed   320.0   250.0\n",
      "            weight  1.0     0.8\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#help을 통해 사용법 확인\n",
    "# 데이터 프레임에서 결측치 drop을 어떻게 사용했더라?\n",
    "print(help(pd.DataFrame.drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bf3d854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:48.235509Z",
     "iopub.status.busy": "2025-06-19T07:24:48.234770Z",
     "iopub.status.idle": "2025-06-19T07:24:48.238682Z",
     "shell.execute_reply": "2025-06-19T07:24:48.239349Z",
     "shell.execute_reply.started": "2025-06-19T07:23:53.986380Z"
    },
    "papermill": {
     "duration": 0.024879,
     "end_time": "2025-06-19T07:24:48.239547",
     "exception": false,
     "start_time": "2025-06-19T07:24:48.214668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BooleanDtype', 'Categorical', 'CategoricalDtype', 'CategoricalIndex', 'DataFrame', 'DateOffset', 'DatetimeIndex', 'DatetimeTZDtype', 'ExcelFile', 'ExcelWriter', 'Flags', 'Float32Dtype', 'Float64Dtype', 'Float64Index', 'Grouper', 'HDFStore', 'Index', 'IndexSlice', 'Int16Dtype', 'Int32Dtype', 'Int64Dtype', 'Int64Index', 'Int8Dtype', 'Interval', 'IntervalDtype', 'IntervalIndex', 'MultiIndex', 'NA', 'NaT', 'NamedAgg', 'Period', 'PeriodDtype', 'PeriodIndex', 'RangeIndex', 'Series', 'SparseDtype', 'StringDtype', 'Timedelta', 'TimedeltaIndex', 'Timestamp', 'UInt16Dtype', 'UInt32Dtype', 'UInt64Dtype', 'UInt64Index', 'UInt8Dtype', '__builtins__', '__cached__', '__doc__', '__docformat__', '__file__', '__getattr__', '__git_version__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_config', '_hashtable', '_is_numpy_dev', '_lib', '_libs', '_np_version_under1p18', '_testing', '_tslib', '_typing', '_version', 'api', 'array', 'arrays', 'bdate_range', 'compat', 'concat', 'core', 'crosstab', 'cut', 'date_range', 'describe_option', 'errors', 'eval', 'factorize', 'get_dummies', 'get_option', 'infer_freq', 'interval_range', 'io', 'isna', 'isnull', 'json_normalize', 'lreshape', 'melt', 'merge', 'merge_asof', 'merge_ordered', 'notna', 'notnull', 'offsets', 'option_context', 'options', 'pandas', 'period_range', 'pivot', 'pivot_table', 'plotting', 'qcut', 'read_clipboard', 'read_csv', 'read_excel', 'read_feather', 'read_fwf', 'read_gbq', 'read_hdf', 'read_html', 'read_json', 'read_orc', 'read_parquet', 'read_pickle', 'read_sas', 'read_spss', 'read_sql', 'read_sql_query', 'read_sql_table', 'read_stata', 'read_table', 'read_xml', 'reset_option', 'set_eng_float_format', 'set_option', 'show_versions', 'test', 'testing', 'timedelta_range', 'to_datetime', 'to_numeric', 'to_pickle', 'to_timedelta', 'tseries', 'unique', 'util', 'value_counts', 'wide_to_long']\n",
      "Help on function get_dummies in module pandas.core.reshape.reshape:\n",
      "\n",
      "get_dummies(data, prefix=None, prefix_sep='_', dummy_na: 'bool' = False, columns=None, sparse: 'bool' = False, drop_first: 'bool' = False, dtype: 'Dtype | None' = None) -> 'DataFrame'\n",
      "    Convert categorical variable into dummy/indicator variables.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : array-like, Series, or DataFrame\n",
      "        Data of which to get dummy indicators.\n",
      "    prefix : str, list of str, or dict of str, default None\n",
      "        String to append DataFrame column names.\n",
      "        Pass a list with length equal to the number of columns\n",
      "        when calling get_dummies on a DataFrame. Alternatively, `prefix`\n",
      "        can be a dictionary mapping column names to prefixes.\n",
      "    prefix_sep : str, default '_'\n",
      "        If appending prefix, separator/delimiter to use. Or pass a\n",
      "        list or dictionary as with `prefix`.\n",
      "    dummy_na : bool, default False\n",
      "        Add a column to indicate NaNs, if False NaNs are ignored.\n",
      "    columns : list-like, default None\n",
      "        Column names in the DataFrame to be encoded.\n",
      "        If `columns` is None then all the columns with\n",
      "        `object` or `category` dtype will be converted.\n",
      "    sparse : bool, default False\n",
      "        Whether the dummy-encoded columns should be backed by\n",
      "        a :class:`SparseArray` (True) or a regular NumPy array (False).\n",
      "    drop_first : bool, default False\n",
      "        Whether to get k-1 dummies out of k categorical levels by removing the\n",
      "        first level.\n",
      "    dtype : dtype, default np.uint8\n",
      "        Data type for new columns. Only a single dtype is allowed.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame\n",
      "        Dummy-coded data.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    Series.str.get_dummies : Convert Series to dummy codes.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> s = pd.Series(list('abca'))\n",
      "    \n",
      "    >>> pd.get_dummies(s)\n",
      "       a  b  c\n",
      "    0  1  0  0\n",
      "    1  0  1  0\n",
      "    2  0  0  1\n",
      "    3  1  0  0\n",
      "    \n",
      "    >>> s1 = ['a', 'b', np.nan]\n",
      "    \n",
      "    >>> pd.get_dummies(s1)\n",
      "       a  b\n",
      "    0  1  0\n",
      "    1  0  1\n",
      "    2  0  0\n",
      "    \n",
      "    >>> pd.get_dummies(s1, dummy_na=True)\n",
      "       a  b  NaN\n",
      "    0  1  0    0\n",
      "    1  0  1    0\n",
      "    2  0  0    1\n",
      "    \n",
      "    >>> df = pd.DataFrame({'A': ['a', 'b', 'a'], 'B': ['b', 'a', 'c'],\n",
      "    ...                    'C': [1, 2, 3]})\n",
      "    \n",
      "    >>> pd.get_dummies(df, prefix=['col1', 'col2'])\n",
      "       C  col1_a  col1_b  col2_a  col2_b  col2_c\n",
      "    0  1       1       0       0       1       0\n",
      "    1  2       0       1       1       0       0\n",
      "    2  3       1       0       0       0       1\n",
      "    \n",
      "    >>> pd.get_dummies(pd.Series(list('abcaa')))\n",
      "       a  b  c\n",
      "    0  1  0  0\n",
      "    1  0  1  0\n",
      "    2  0  0  1\n",
      "    3  1  0  0\n",
      "    4  1  0  0\n",
      "    \n",
      "    >>> pd.get_dummies(pd.Series(list('abcaa')), drop_first=True)\n",
      "       b  c\n",
      "    0  0  0\n",
      "    1  1  0\n",
      "    2  0  1\n",
      "    3  0  0\n",
      "    4  0  0\n",
      "    \n",
      "    >>> pd.get_dummies(pd.Series(list('abc')), dtype=float)\n",
      "         a    b    c\n",
      "    0  1.0  0.0  0.0\n",
      "    1  0.0  1.0  0.0\n",
      "    2  0.0  0.0  1.0\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#원핫인코딩 어떻게 사용했더라?\n",
    "import pandas as pd\n",
    "print(dir(pd))\n",
    "print(help(pd.get_dummies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82802fef",
   "metadata": {
    "papermill": {
     "duration": 0.015507,
     "end_time": "2025-06-19T07:24:48.270369",
     "exception": false,
     "start_time": "2025-06-19T07:24:48.254862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Sklearn\n",
    "- 판다스에서 dir을 주로 활용했다면 사이킷런에선 \\_\\_all__을 활용해요!\n",
    "- 주의 할 점은 캐글 노트북 환경에서는 import sklearn만 호출해도 모든 라이브러리를 동작할 수 있지만 시험환경에서는 각각 세부 라이브러리를 임포트 해줘야 \\_\\_all__'또는 help, dir을 활용할 때 에러가 발생되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853c7e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:48.308389Z",
     "iopub.status.busy": "2025-06-19T07:24:48.307542Z",
     "iopub.status.idle": "2025-06-19T07:24:49.223761Z",
     "shell.execute_reply": "2025-06-19T07:24:49.223208Z",
     "shell.execute_reply.started": "2025-06-19T07:24:09.960846Z"
    },
    "papermill": {
     "duration": 0.937189,
     "end_time": "2025-06-19T07:24:49.223910",
     "exception": false,
     "start_time": "2025-06-19T07:24:48.286721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calibration', 'cluster', 'covariance', 'cross_decomposition', 'datasets', 'decomposition', 'dummy', 'ensemble', 'exceptions', 'experimental', 'externals', 'feature_extraction', 'feature_selection', 'gaussian_process', 'inspection', 'isotonic', 'kernel_approximation', 'kernel_ridge', 'linear_model', 'manifold', 'metrics', 'mixture', 'model_selection', 'multiclass', 'multioutput', 'naive_bayes', 'neighbors', 'neural_network', 'pipeline', 'preprocessing', 'random_projection', 'semi_supervised', 'svm', 'tree', 'discriminant_analysis', 'impute', 'compose', 'clone', 'get_config', 'set_config', 'config_context', 'show_versions']\n"
     ]
    }
   ],
   "source": [
    "#sklearn까지만 외워주세요 :) \n",
    "import sklearn\n",
    "print(sklearn.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be636a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:49.259962Z",
     "iopub.status.busy": "2025-06-19T07:24:49.259253Z",
     "iopub.status.idle": "2025-06-19T07:24:49.295928Z",
     "shell.execute_reply": "2025-06-19T07:24:49.295331Z",
     "shell.execute_reply.started": "2025-06-19T07:07:43.954743Z"
    },
    "papermill": {
     "duration": 0.056667,
     "end_time": "2025-06-19T07:24:49.296095",
     "exception": false,
     "start_time": "2025-06-19T07:24:49.239428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Binarizer', 'FunctionTransformer', 'KBinsDiscretizer', 'KernelCenterer', 'LabelBinarizer', 'LabelEncoder', 'MultiLabelBinarizer', 'MinMaxScaler', 'MaxAbsScaler', 'QuantileTransformer', 'Normalizer', 'OneHotEncoder', 'OrdinalEncoder', 'PowerTransformer', 'RobustScaler', 'StandardScaler', 'add_dummy_feature', 'PolynomialFeatures', 'binarize', 'normalize', 'scale', 'robust_scale', 'maxabs_scale', 'minmax_scale', 'label_binarize', 'quantile_transform', 'power_transform']\n"
     ]
    }
   ],
   "source": [
    "# 전처리 무엇을 할 수 있지?\n",
    "import sklearn.preprocessing\n",
    "print(sklearn.preprocessing.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9bf73c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:49.336310Z",
     "iopub.status.busy": "2025-06-19T07:24:49.335448Z",
     "iopub.status.idle": "2025-06-19T07:24:49.342909Z",
     "shell.execute_reply": "2025-06-19T07:24:49.343710Z",
     "shell.execute_reply.started": "2025-06-19T07:07:33.799483Z"
    },
    "papermill": {
     "duration": 0.031255,
     "end_time": "2025-06-19T07:24:49.343954",
     "exception": false,
     "start_time": "2025-06-19T07:24:49.312699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MinMaxScaler in module sklearn.preprocessing._data:\n",
      "\n",
      "class MinMaxScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  MinMaxScaler(feature_range=(0, 1), *, copy=True)\n",
      " |  \n",
      " |  Transform features by scaling each feature to a given range.\n",
      " |  \n",
      " |  This estimator scales and translates each feature individually such\n",
      " |  that it is in the given range on the training set, e.g. between\n",
      " |  zero and one.\n",
      " |  \n",
      " |  The transformation is given by::\n",
      " |  \n",
      " |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      " |      X_scaled = X_std * (max - min) + min\n",
      " |  \n",
      " |  where min, max = feature_range.\n",
      " |  \n",
      " |  This transformation is often used as an alternative to zero mean,\n",
      " |  unit variance scaling.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  feature_range : tuple (min, max), default=(0, 1)\n",
      " |      Desired range of transformed data.\n",
      " |  \n",
      " |  copy : bool, default=True\n",
      " |      Set to False to perform inplace row normalization and avoid a\n",
      " |      copy (if the input is already a numpy array).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  min_ : ndarray of shape (n_features,)\n",
      " |      Per feature adjustment for minimum. Equivalent to\n",
      " |      ``min - X.min(axis=0) * self.scale_``\n",
      " |  \n",
      " |  scale_ : ndarray of shape (n_features,)\n",
      " |      Per feature relative scaling of the data. Equivalent to\n",
      " |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  data_min_ : ndarray of shape (n_features,)\n",
      " |      Per feature minimum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_min_*\n",
      " |  \n",
      " |  data_max_ : ndarray of shape (n_features,)\n",
      " |      Per feature maximum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_max_*\n",
      " |  \n",
      " |  data_range_ : ndarray of shape (n_features,)\n",
      " |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_range_*\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator.\n",
      " |      It will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      " |  >>> scaler = MinMaxScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  MinMaxScaler()\n",
      " |  >>> print(scaler.data_max_)\n",
      " |  [ 1. 18.]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[0.   0.  ]\n",
      " |   [0.25 0.25]\n",
      " |   [0.5  0.5 ]\n",
      " |   [1.   1.  ]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[1.5 0. ]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  minmax_scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MinMaxScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, feature_range=(0, 1), *, copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the minimum and maximum to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the per-feature minimum and maximum\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Undo the scaling of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed. It cannot be sparse.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of min and max on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Transformer instance.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scale features of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 문제에서 민맥스스케일을 적용하라고 하네. 어떻게 사용하지?\n",
    "import sklearn.preprocessing\n",
    "print(help(sklearn.preprocessing.MinMaxScaler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b2743d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:49.383626Z",
     "iopub.status.busy": "2025-06-19T07:24:49.382877Z",
     "iopub.status.idle": "2025-06-19T07:24:49.454697Z",
     "shell.execute_reply": "2025-06-19T07:24:49.454050Z",
     "shell.execute_reply.started": "2025-06-19T07:08:03.291978Z"
    },
    "papermill": {
     "duration": 0.093843,
     "end_time": "2025-06-19T07:24:49.454839",
     "exception": false,
     "start_time": "2025-06-19T07:24:49.360996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BaseCrossValidator', 'GridSearchCV', 'TimeSeriesSplit', 'KFold', 'GroupKFold', 'GroupShuffleSplit', 'LeaveOneGroupOut', 'LeaveOneOut', 'LeavePGroupsOut', 'LeavePOut', 'RepeatedKFold', 'RepeatedStratifiedKFold', 'ParameterGrid', 'ParameterSampler', 'PredefinedSplit', 'RandomizedSearchCV', 'ShuffleSplit', 'StratifiedKFold', 'StratifiedShuffleSplit', 'check_cv', 'cross_val_predict', 'cross_val_score', 'cross_validate', 'fit_grid_point', 'learning_curve', 'permutation_test_score', 'train_test_split', 'validation_curve')\n"
     ]
    }
   ],
   "source": [
    "# 데이터를 나눠야 하는데 풀네임이 뭐더라?\n",
    "# 데이터를 트레인과 테스트로 나눠야할 떼 model_selection안에 있다는건 아셔야 해요^^\n",
    "import sklearn.model_selection\n",
    "print(sklearn.model_selection.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9ae698",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:49.495193Z",
     "iopub.status.busy": "2025-06-19T07:24:49.494303Z",
     "iopub.status.idle": "2025-06-19T07:24:49.497973Z",
     "shell.execute_reply": "2025-06-19T07:24:49.498654Z",
     "shell.execute_reply.started": "2025-06-19T07:12:34.161396Z"
    },
    "papermill": {
     "duration": 0.026704,
     "end_time": "2025-06-19T07:24:49.498840",
     "exception": false,
     "start_time": "2025-06-19T07:24:49.472136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, **options)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int or RandomState instance, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 어떻께 사용하더라??\n",
    "import sklearn.model_selection\n",
    "print(help(sklearn.model_selection.train_test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "921d41a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:49.537925Z",
     "iopub.status.busy": "2025-06-19T07:24:49.537059Z",
     "iopub.status.idle": "2025-06-19T07:24:49.761848Z",
     "shell.execute_reply": "2025-06-19T07:24:49.761081Z",
     "shell.execute_reply.started": "2021-11-26T03:00:52.973727Z"
    },
    "papermill": {
     "duration": 0.246288,
     "end_time": "2025-06-19T07:24:49.762001",
     "exception": false,
     "start_time": "2025-06-19T07:24:49.515713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BaseEnsemble', 'RandomForestClassifier', 'RandomForestRegressor', 'RandomTreesEmbedding', 'ExtraTreesClassifier', 'ExtraTreesRegressor', 'BaggingClassifier', 'BaggingRegressor', 'IsolationForest', 'GradientBoostingClassifier', 'GradientBoostingRegressor', 'AdaBoostClassifier', 'AdaBoostRegressor', 'VotingClassifier', 'VotingRegressor', 'StackingClassifier', 'StackingRegressor']\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 모델 쓸래!\n",
    "import sklearn.ensemble\n",
    "print(sklearn.ensemble.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3067d0ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:49.807615Z",
     "iopub.status.busy": "2025-06-19T07:24:49.804820Z",
     "iopub.status.idle": "2025-06-19T07:24:49.812638Z",
     "shell.execute_reply": "2025-06-19T07:24:49.813352Z",
     "shell.execute_reply.started": "2025-06-19T07:18:39.331194Z"
    },
    "papermill": {
     "duration": 0.034151,
     "end_time": "2025-06-19T07:24:49.813626",
     "exception": false,
     "start_time": "2025-06-19T07:24:49.779475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RandomForestClassifier in module sklearn.ensemble._forest object:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate\n",
      " |      the generalization accuracy.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int or RandomState, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0, 1)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 랜덤포레스트 어떻게 썻더라?\n",
    "import sklearn.ensemble\n",
    "print(help(sklearn.ensemble.RandomForestClassifier()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9089546",
   "metadata": {
    "papermill": {
     "duration": 0.018879,
     "end_time": "2025-06-19T07:24:49.853325",
     "exception": false,
     "start_time": "2025-06-19T07:24:49.834446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 당황하지 말고, 공식문서 통으로 만들기\n",
    "- 실제 시험환경에서는 1000줄만 출력되어 사용하기 어려움을 확인함. 위에 개별 help()함수를 통해 확인하는 것을 권장합니다.\n",
    "- 출력이 길어서 주석 처리함! \n",
    "- 해당 출력물을 메모장에 복사한 뒤 검색 기능을 활용에 문서 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a42efa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:50.014470Z",
     "iopub.status.busy": "2025-06-19T07:24:49.915916Z",
     "iopub.status.idle": "2025-06-19T07:24:50.046546Z",
     "shell.execute_reply": "2025-06-19T07:24:50.047375Z"
    },
    "papermill": {
     "duration": 0.175642,
     "end_time": "2025-06-19T07:24:50.047649",
     "exception": false,
     "start_time": "2025-06-19T07:24:49.872007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############### Binarizer ################\n",
      "Help on class Binarizer in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.Binarizer = class Binarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.Binarizer(*, threshold=0.0, copy=True)\n",
      " |  \n",
      " |  Binarize data (set feature values to 0 or 1) according to a threshold\n",
      " |  \n",
      " |  Values greater than the threshold map to 1, while values less than\n",
      " |  or equal to the threshold map to 0. With the default threshold of 0,\n",
      " |  only positive values map to 1.\n",
      " |  \n",
      " |  Binarization is a common operation on text count data where the\n",
      " |  analyst can decide to only consider the presence or absence of a\n",
      " |  feature rather than a quantified number of occurrences for instance.\n",
      " |  \n",
      " |  It can also be used as a pre-processing step for estimators that\n",
      " |  consider boolean random variables (e.g. modelled using the Bernoulli\n",
      " |  distribution in a Bayesian setting).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_binarization>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  threshold : float, optional (0.0 by default)\n",
      " |      Feature values below or equal to this are replaced by 0, above it by 1.\n",
      " |      Threshold may not be less than 0 for operations on sparse matrices.\n",
      " |  \n",
      " |  copy : boolean, optional, default True\n",
      " |      set to False to perform inplace binarization and avoid a copy (if\n",
      " |      the input is already a numpy array or a scipy.sparse CSR matrix).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import Binarizer\n",
      " |  >>> X = [[ 1., -1.,  2.],\n",
      " |  ...      [ 2.,  0.,  0.],\n",
      " |  ...      [ 0.,  1., -1.]]\n",
      " |  >>> transformer = Binarizer().fit(X)  # fit does nothing.\n",
      " |  >>> transformer\n",
      " |  Binarizer()\n",
      " |  >>> transformer.transform(X)\n",
      " |  array([[1., 0., 1.],\n",
      " |         [1., 0., 0.],\n",
      " |         [0., 1., 0.]])\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  If the input is a sparse matrix, only the non-zero values are subject\n",
      " |  to update by the Binarizer class.\n",
      " |  \n",
      " |  This estimator is stateless (besides constructor parameters), the\n",
      " |  fit method does nothing but is useful when used in a pipeline.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  binarize: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Binarizer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, threshold=0.0, copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Do nothing and return the estimator unchanged\n",
      " |      \n",
      " |      This method is just there to implement the usual API and hence\n",
      " |      work in pipelines.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Binarize each element of X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data to binarize, element by element.\n",
      " |          scipy.sparse matrices should be in CSR format to avoid an\n",
      " |          un-necessary copy.\n",
      " |      \n",
      " |      copy : bool\n",
      " |          Copy the input X or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### FunctionTransformer ################\n",
      "Help on class FunctionTransformer in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.FunctionTransformer = class FunctionTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.FunctionTransformer(func=None, inverse_func=None, *, validate=False, accept_sparse=False, check_inverse=True, kw_args=None, inv_kw_args=None)\n",
      " |  \n",
      " |  Constructs a transformer from an arbitrary callable.\n",
      " |  \n",
      " |  A FunctionTransformer forwards its X (and optionally y) arguments to a\n",
      " |  user-defined function or function object and returns the result of this\n",
      " |  function. This is useful for stateless transformations such as taking the\n",
      " |  log of frequencies, doing custom scaling, etc.\n",
      " |  \n",
      " |  Note: If a lambda is used as the function, then the resulting\n",
      " |  transformer will not be pickleable.\n",
      " |  \n",
      " |  .. versionadded:: 0.17\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <function_transformer>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  func : callable, optional default=None\n",
      " |      The callable to use for the transformation. This will be passed\n",
      " |      the same arguments as transform, with args and kwargs forwarded.\n",
      " |      If func is None, then func will be the identity function.\n",
      " |  \n",
      " |  inverse_func : callable, optional default=None\n",
      " |      The callable to use for the inverse transformation. This will be\n",
      " |      passed the same arguments as inverse transform, with args and\n",
      " |      kwargs forwarded. If inverse_func is None, then inverse_func\n",
      " |      will be the identity function.\n",
      " |  \n",
      " |  validate : bool, optional default=False\n",
      " |      Indicate that the input X array should be checked before calling\n",
      " |      ``func``. The possibilities are:\n",
      " |  \n",
      " |      - If False, there is no input validation.\n",
      " |      - If True, then X will be converted to a 2-dimensional NumPy array or\n",
      " |        sparse matrix. If the conversion is not possible an exception is\n",
      " |        raised.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default of ``validate`` changed from True to False.\n",
      " |  \n",
      " |  accept_sparse : boolean, optional\n",
      " |      Indicate that func accepts a sparse matrix as input. If validate is\n",
      " |      False, this has no effect. Otherwise, if accept_sparse is false,\n",
      " |      sparse matrix inputs will cause an exception to be raised.\n",
      " |  \n",
      " |  check_inverse : bool, default=True\n",
      " |     Whether to check that or ``func`` followed by ``inverse_func`` leads to\n",
      " |     the original inputs. It can be used for a sanity check, raising a\n",
      " |     warning when the condition is not fulfilled.\n",
      " |  \n",
      " |     .. versionadded:: 0.20\n",
      " |  \n",
      " |  kw_args : dict, optional\n",
      " |      Dictionary of additional keyword arguments to pass to func.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  inv_kw_args : dict, optional\n",
      " |      Dictionary of additional keyword arguments to pass to inverse_func.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.preprocessing import FunctionTransformer\n",
      " |  >>> transformer = FunctionTransformer(np.log1p)\n",
      " |  >>> X = np.array([[0, 1], [2, 3]])\n",
      " |  >>> transformer.transform(X)\n",
      " |  array([[0.       , 0.6931...],\n",
      " |         [1.0986..., 1.3862...]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FunctionTransformer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, func=None, inverse_func=None, *, validate=False, accept_sparse=False, check_inverse=True, kw_args=None, inv_kw_args=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit transformer by checking X.\n",
      " |      \n",
      " |      If ``validate`` is ``True``, ``X`` will be checked.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Input array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Transform X using the inverse function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Input array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : array-like, shape (n_samples, n_features)\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X using the forward function.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          Input array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : array-like, shape (n_samples, n_features)\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### KBinsDiscretizer ################\n",
      "Help on class KBinsDiscretizer in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.KBinsDiscretizer = class KBinsDiscretizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.KBinsDiscretizer(n_bins=5, *, encode='onehot', strategy='quantile')\n",
      " |  \n",
      " |  Bin continuous data into intervals.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_discretization>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.20\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_bins : int or array-like, shape (n_features,) (default=5)\n",
      " |      The number of bins to produce. Raises ValueError if ``n_bins < 2``.\n",
      " |  \n",
      " |  encode : {'onehot', 'onehot-dense', 'ordinal'}, (default='onehot')\n",
      " |      Method used to encode the transformed result.\n",
      " |  \n",
      " |      onehot\n",
      " |          Encode the transformed result with one-hot encoding\n",
      " |          and return a sparse matrix. Ignored features are always\n",
      " |          stacked to the right.\n",
      " |      onehot-dense\n",
      " |          Encode the transformed result with one-hot encoding\n",
      " |          and return a dense array. Ignored features are always\n",
      " |          stacked to the right.\n",
      " |      ordinal\n",
      " |          Return the bin identifier encoded as an integer value.\n",
      " |  \n",
      " |  strategy : {'uniform', 'quantile', 'kmeans'}, (default='quantile')\n",
      " |      Strategy used to define the widths of the bins.\n",
      " |  \n",
      " |      uniform\n",
      " |          All bins in each feature have identical widths.\n",
      " |      quantile\n",
      " |          All bins in each feature have the same number of points.\n",
      " |      kmeans\n",
      " |          Values in each bin have the same nearest center of a 1D k-means\n",
      " |          cluster.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_bins_ : int array, shape (n_features,)\n",
      " |      Number of bins per feature. Bins whose width are too small\n",
      " |      (i.e., <= 1e-8) are removed with a warning.\n",
      " |  \n",
      " |  bin_edges_ : array of arrays, shape (n_features, )\n",
      " |      The edges of each bin. Contain arrays of varying shapes ``(n_bins_, )``\n",
      " |      Ignored features will have empty arrays.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |   sklearn.preprocessing.Binarizer : Class used to bin values as ``0`` or\n",
      " |      ``1`` based on a parameter ``threshold``.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  In bin edges for feature ``i``, the first and last values are used only for\n",
      " |  ``inverse_transform``. During transform, bin edges are extended to::\n",
      " |  \n",
      " |    np.concatenate([-np.inf, bin_edges_[i][1:-1], np.inf])\n",
      " |  \n",
      " |  You can combine ``KBinsDiscretizer`` with\n",
      " |  :class:`sklearn.compose.ColumnTransformer` if you only want to preprocess\n",
      " |  part of the features.\n",
      " |  \n",
      " |  ``KBinsDiscretizer`` might produce constant features (e.g., when\n",
      " |  ``encode = 'onehot'`` and certain bins do not contain any data).\n",
      " |  These features can be removed with feature selection algorithms\n",
      " |  (e.g., :class:`sklearn.feature_selection.VarianceThreshold`).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[-2, 1, -4,   -1],\n",
      " |  ...      [-1, 2, -3, -0.5],\n",
      " |  ...      [ 0, 3, -2,  0.5],\n",
      " |  ...      [ 1, 4, -1,    2]]\n",
      " |  >>> est = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
      " |  >>> est.fit(X)\n",
      " |  KBinsDiscretizer(...)\n",
      " |  >>> Xt = est.transform(X)\n",
      " |  >>> Xt  # doctest: +SKIP\n",
      " |  array([[ 0., 0., 0., 0.],\n",
      " |         [ 1., 1., 1., 0.],\n",
      " |         [ 2., 2., 2., 1.],\n",
      " |         [ 2., 2., 2., 2.]])\n",
      " |  \n",
      " |  Sometimes it may be useful to convert the data back into the original\n",
      " |  feature space. The ``inverse_transform`` function converts the binned\n",
      " |  data into the original feature space. Each value will be equal to the mean\n",
      " |  of the two bin edges.\n",
      " |  \n",
      " |  >>> est.bin_edges_[0]\n",
      " |  array([-2., -1.,  0.,  1.])\n",
      " |  >>> est.inverse_transform(Xt)\n",
      " |  array([[-1.5,  1.5, -3.5, -0.5],\n",
      " |         [-0.5,  2.5, -2.5, -0.5],\n",
      " |         [ 0.5,  3.5, -1.5,  0.5],\n",
      " |         [ 0.5,  3.5, -1.5,  1.5]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KBinsDiscretizer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_bins=5, *, encode='onehot', strategy='quantile')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numeric array-like, shape (n_samples, n_features)\n",
      " |          Data to be discretized.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  inverse_transform(self, Xt)\n",
      " |      Transform discretized data back to original feature space.\n",
      " |      \n",
      " |      Note that this function does not regenerate the original data\n",
      " |      due to discretization rounding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Xt : numeric array-like, shape (n_sample, n_features)\n",
      " |          Transformed data in the binned space.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xinv : numeric array-like\n",
      " |          Data in the original feature space.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Discretize the data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : numeric array-like, shape (n_samples, n_features)\n",
      " |          Data to be discretized.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : numeric array-like or sparse matrix\n",
      " |          Data in the binned space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### KernelCenterer ################\n",
      "Help on class KernelCenterer in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.KernelCenterer = class KernelCenterer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  Center a kernel matrix\n",
      " |  \n",
      " |  Let K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a\n",
      " |  function mapping x to a Hilbert space. KernelCenterer centers (i.e.,\n",
      " |  normalize to have zero mean) the data without explicitly computing phi(x).\n",
      " |  It is equivalent to centering phi(x) with\n",
      " |  sklearn.preprocessing.StandardScaler(with_std=False).\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <kernel_centering>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  K_fit_rows_ : array, shape (n_samples,)\n",
      " |      Average of each column of kernel matrix\n",
      " |  \n",
      " |  K_fit_all_ : float\n",
      " |      Average of kernel matrix\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import KernelCenterer\n",
      " |  >>> from sklearn.metrics.pairwise import pairwise_kernels\n",
      " |  >>> X = [[ 1., -2.,  2.],\n",
      " |  ...      [ -2.,  1.,  3.],\n",
      " |  ...      [ 4.,  1., -2.]]\n",
      " |  >>> K = pairwise_kernels(X, metric='linear')\n",
      " |  >>> K\n",
      " |  array([[  9.,   2.,  -2.],\n",
      " |         [  2.,  14., -13.],\n",
      " |         [ -2., -13.,  21.]])\n",
      " |  >>> transformer = KernelCenterer().fit(K)\n",
      " |  >>> transformer\n",
      " |  KernelCenterer()\n",
      " |  >>> transformer.transform(K)\n",
      " |  array([[  5.,   0.,  -5.],\n",
      " |         [  0.,  14., -14.],\n",
      " |         [ -5., -14.,  19.]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KernelCenterer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, K, y=None)\n",
      " |      Fit KernelCenterer\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      K : numpy array of shape [n_samples, n_samples]\n",
      " |          Kernel matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  transform(self, K, copy=True)\n",
      " |      Center kernel matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      K : numpy array of shape [n_samples1, n_samples2]\n",
      " |          Kernel matrix.\n",
      " |      \n",
      " |      copy : boolean, optional, default True\n",
      " |          Set to False to perform inplace computation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      K_new : numpy array of shape [n_samples1, n_samples2]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### LabelBinarizer ################\n",
      "Help on class LabelBinarizer in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.LabelBinarizer = class LabelBinarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.LabelBinarizer(*, neg_label=0, pos_label=1, sparse_output=False)\n",
      " |  \n",
      " |  Binarize labels in a one-vs-all fashion\n",
      " |  \n",
      " |  Several regression and binary classification algorithms are\n",
      " |  available in scikit-learn. A simple way to extend these algorithms\n",
      " |  to the multi-class classification case is to use the so-called\n",
      " |  one-vs-all scheme.\n",
      " |  \n",
      " |  At learning time, this simply consists in learning one regressor\n",
      " |  or binary classifier per class. In doing so, one needs to convert\n",
      " |  multi-class labels to binary labels (belong or does not belong\n",
      " |  to the class). LabelBinarizer makes this process easy with the\n",
      " |  transform method.\n",
      " |  \n",
      " |  At prediction time, one assigns the class for which the corresponding\n",
      " |  model gave the greatest confidence. LabelBinarizer makes this easy\n",
      " |  with the inverse_transform method.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  neg_label : int (default: 0)\n",
      " |      Value with which negative labels must be encoded.\n",
      " |  \n",
      " |  pos_label : int (default: 1)\n",
      " |      Value with which positive labels must be encoded.\n",
      " |  \n",
      " |  sparse_output : boolean (default: False)\n",
      " |      True if the returned array from transform is desired to be in sparse\n",
      " |      CSR format.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : array of shape [n_class]\n",
      " |      Holds the label for each class.\n",
      " |  \n",
      " |  y_type_ : str,\n",
      " |      Represents the type of the target data as evaluated by\n",
      " |      utils.multiclass.type_of_target. Possible type are 'continuous',\n",
      " |      'continuous-multioutput', 'binary', 'multiclass',\n",
      " |      'multiclass-multioutput', 'multilabel-indicator', and 'unknown'.\n",
      " |  \n",
      " |  sparse_input_ : boolean,\n",
      " |      True if the input data to transform is given as a sparse matrix, False\n",
      " |      otherwise.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn import preprocessing\n",
      " |  >>> lb = preprocessing.LabelBinarizer()\n",
      " |  >>> lb.fit([1, 2, 6, 4, 2])\n",
      " |  LabelBinarizer()\n",
      " |  >>> lb.classes_\n",
      " |  array([1, 2, 4, 6])\n",
      " |  >>> lb.transform([1, 6])\n",
      " |  array([[1, 0, 0, 0],\n",
      " |         [0, 0, 0, 1]])\n",
      " |  \n",
      " |  Binary targets transform to a column vector\n",
      " |  \n",
      " |  >>> lb = preprocessing.LabelBinarizer()\n",
      " |  >>> lb.fit_transform(['yes', 'no', 'no', 'yes'])\n",
      " |  array([[1],\n",
      " |         [0],\n",
      " |         [0],\n",
      " |         [1]])\n",
      " |  \n",
      " |  Passing a 2D matrix for multilabel classification\n",
      " |  \n",
      " |  >>> import numpy as np\n",
      " |  >>> lb.fit(np.array([[0, 1, 1], [1, 0, 0]]))\n",
      " |  LabelBinarizer()\n",
      " |  >>> lb.classes_\n",
      " |  array([0, 1, 2])\n",
      " |  >>> lb.transform([0, 1, 2, 1])\n",
      " |  array([[1, 0, 0],\n",
      " |         [0, 1, 0],\n",
      " |         [0, 0, 1],\n",
      " |         [0, 1, 0]])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  label_binarize : function to perform the transform operation of\n",
      " |      LabelBinarizer with fixed classes.\n",
      " |  sklearn.preprocessing.OneHotEncoder : encode categorical features\n",
      " |      using a one-hot aka one-of-K scheme.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LabelBinarizer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, neg_label=0, pos_label=1, sparse_output=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, y)\n",
      " |      Fit label binarizer\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array of shape [n_samples,] or [n_samples, n_classes]\n",
      " |          Target values. The 2-d matrix should only contain 0 and 1,\n",
      " |          represents multilabel classification.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  fit_transform(self, y)\n",
      " |      Fit label binarizer and transform multi-class labels to binary\n",
      " |      labels.\n",
      " |      \n",
      " |      The output of transform is sometimes referred to as\n",
      " |      the 1-of-K coding scheme.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array or sparse matrix of shape [n_samples,] or             [n_samples, n_classes]\n",
      " |          Target values. The 2-d matrix should only contain 0 and 1,\n",
      " |          represents multilabel classification. Sparse matrix can be\n",
      " |          CSR, CSC, COO, DOK, or LIL.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Y : array or CSR matrix of shape [n_samples, n_classes]\n",
      " |          Shape will be [n_samples, 1] for binary problems.\n",
      " |  \n",
      " |  inverse_transform(self, Y, threshold=None)\n",
      " |      Transform binary labels back to multi-class labels\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Y : numpy array or sparse matrix with shape [n_samples, n_classes]\n",
      " |          Target values. All sparse matrices are converted to CSR before\n",
      " |          inverse transformation.\n",
      " |      \n",
      " |      threshold : float or None\n",
      " |          Threshold used in the binary and multi-label cases.\n",
      " |      \n",
      " |          Use 0 when ``Y`` contains the output of decision_function\n",
      " |          (classifier).\n",
      " |          Use 0.5 when ``Y`` contains the output of predict_proba.\n",
      " |      \n",
      " |          If None, the threshold is assumed to be half way between\n",
      " |          neg_label and pos_label.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : numpy array or CSR matrix of shape [n_samples] Target values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      In the case when the binary labels are fractional\n",
      " |      (probabilistic), inverse_transform chooses the class with the\n",
      " |      greatest value. Typically, this allows to use the output of a\n",
      " |      linear model's decision_function method directly as the input\n",
      " |      of inverse_transform.\n",
      " |  \n",
      " |  transform(self, y)\n",
      " |      Transform multi-class labels to binary labels\n",
      " |      \n",
      " |      The output of transform is sometimes referred to by some authors as\n",
      " |      the 1-of-K coding scheme.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array or sparse matrix of shape [n_samples,] or             [n_samples, n_classes]\n",
      " |          Target values. The 2-d matrix should only contain 0 and 1,\n",
      " |          represents multilabel classification. Sparse matrix can be\n",
      " |          CSR, CSC, COO, DOK, or LIL.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Y : numpy array or CSR matrix of shape [n_samples, n_classes]\n",
      " |          Shape will be [n_samples, 1] for binary problems.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### LabelEncoder ################\n",
      "Help on class LabelEncoder in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.LabelEncoder = class LabelEncoder(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  Encode target labels with value between 0 and n_classes-1.\n",
      " |  \n",
      " |  This transformer should be used to encode target values, *i.e.* `y`, and\n",
      " |  not the input `X`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.12\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of shape (n_class,)\n",
      " |      Holds the label for each class.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  `LabelEncoder` can be used to normalize labels.\n",
      " |  \n",
      " |  >>> from sklearn import preprocessing\n",
      " |  >>> le = preprocessing.LabelEncoder()\n",
      " |  >>> le.fit([1, 2, 2, 6])\n",
      " |  LabelEncoder()\n",
      " |  >>> le.classes_\n",
      " |  array([1, 2, 6])\n",
      " |  >>> le.transform([1, 1, 2, 6])\n",
      " |  array([0, 0, 1, 2]...)\n",
      " |  >>> le.inverse_transform([0, 0, 1, 2])\n",
      " |  array([1, 1, 2, 6])\n",
      " |  \n",
      " |  It can also be used to transform non-numerical labels (as long as they are\n",
      " |  hashable and comparable) to numerical labels.\n",
      " |  \n",
      " |  >>> le = preprocessing.LabelEncoder()\n",
      " |  >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
      " |  LabelEncoder()\n",
      " |  >>> list(le.classes_)\n",
      " |  ['amsterdam', 'paris', 'tokyo']\n",
      " |  >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
      " |  array([2, 2, 1]...)\n",
      " |  >>> list(le.inverse_transform([2, 2, 1]))\n",
      " |  ['tokyo', 'tokyo', 'paris']\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OrdinalEncoder : Encode categorical features\n",
      " |      using an ordinal encoding scheme.\n",
      " |  \n",
      " |  sklearn.preprocessing.OneHotEncoder : Encode categorical features\n",
      " |      as a one-hot numeric array.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LabelEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, y)\n",
      " |      Fit label encoder\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  fit_transform(self, y)\n",
      " |      Fit label encoder and return encoded labels\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |  \n",
      " |  inverse_transform(self, y)\n",
      " |      Transform labels back to original encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : numpy array of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : numpy array of shape [n_samples]\n",
      " |  \n",
      " |  transform(self, y)\n",
      " |      Transform labels to normalized encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape [n_samples]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### MultiLabelBinarizer ################\n",
      "Help on class MultiLabelBinarizer in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.MultiLabelBinarizer = class MultiLabelBinarizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.MultiLabelBinarizer(*, classes=None, sparse_output=False)\n",
      " |  \n",
      " |  Transform between iterable of iterables and a multilabel format\n",
      " |  \n",
      " |  Although a list of sets or tuples is a very intuitive format for multilabel\n",
      " |  data, it is unwieldy to process. This transformer converts between this\n",
      " |  intuitive format and the supported multilabel format: a (samples x classes)\n",
      " |  binary matrix indicating the presence of a class label.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  classes : array-like of shape [n_classes] (optional)\n",
      " |      Indicates an ordering for the class labels.\n",
      " |      All entries should be unique (cannot contain duplicate classes).\n",
      " |  \n",
      " |  sparse_output : boolean (default: False),\n",
      " |      Set to true if output binary array is desired in CSR sparse format\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of labels\n",
      " |      A copy of the `classes` parameter where provided,\n",
      " |      or otherwise, the sorted set of classes found when fitting.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MultiLabelBinarizer\n",
      " |  >>> mlb = MultiLabelBinarizer()\n",
      " |  >>> mlb.fit_transform([(1, 2), (3,)])\n",
      " |  array([[1, 1, 0],\n",
      " |         [0, 0, 1]])\n",
      " |  >>> mlb.classes_\n",
      " |  array([1, 2, 3])\n",
      " |  \n",
      " |  >>> mlb.fit_transform([{'sci-fi', 'thriller'}, {'comedy'}])\n",
      " |  array([[0, 1, 1],\n",
      " |         [1, 0, 0]])\n",
      " |  >>> list(mlb.classes_)\n",
      " |  ['comedy', 'sci-fi', 'thriller']\n",
      " |  \n",
      " |  A common mistake is to pass in a list, which leads to the following issue:\n",
      " |  \n",
      " |  >>> mlb = MultiLabelBinarizer()\n",
      " |  >>> mlb.fit(['sci-fi', 'thriller', 'comedy'])\n",
      " |  MultiLabelBinarizer()\n",
      " |  >>> mlb.classes_\n",
      " |  array(['-', 'c', 'd', 'e', 'f', 'h', 'i', 'l', 'm', 'o', 'r', 's', 't',\n",
      " |      'y'], dtype=object)\n",
      " |  \n",
      " |  To correct this, the list of labels should be passed in as:\n",
      " |  \n",
      " |  >>> mlb = MultiLabelBinarizer()\n",
      " |  >>> mlb.fit([['sci-fi', 'thriller', 'comedy']])\n",
      " |  MultiLabelBinarizer()\n",
      " |  >>> mlb.classes_\n",
      " |  array(['comedy', 'sci-fi', 'thriller'], dtype=object)\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OneHotEncoder : encode categorical features\n",
      " |      using a one-hot aka one-of-K scheme.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MultiLabelBinarizer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, classes=None, sparse_output=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, y)\n",
      " |      Fit the label sets binarizer, storing :term:`classes_`\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : iterable of iterables\n",
      " |          A set of labels (any orderable and hashable object) for each\n",
      " |          sample. If the `classes` parameter is set, `y` will not be\n",
      " |          iterated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns this MultiLabelBinarizer instance\n",
      " |  \n",
      " |  fit_transform(self, y)\n",
      " |      Fit the label sets binarizer and transform the given label sets\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : iterable of iterables\n",
      " |          A set of labels (any orderable and hashable object) for each\n",
      " |          sample. If the `classes` parameter is set, `y` will not be\n",
      " |          iterated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_indicator : array or CSR matrix, shape (n_samples, n_classes)\n",
      " |          A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in\n",
      " |          `y[i]`, and 0 otherwise.\n",
      " |  \n",
      " |  inverse_transform(self, yt)\n",
      " |      Transform the given indicator matrix into label sets\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      yt : array or sparse matrix of shape (n_samples, n_classes)\n",
      " |          A matrix containing only 1s ands 0s.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : list of tuples\n",
      " |          The set of labels for each sample such that `y[i]` consists of\n",
      " |          `classes_[j]` for each `yt[i, j] == 1`.\n",
      " |  \n",
      " |  transform(self, y)\n",
      " |      Transform the given label sets\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : iterable of iterables\n",
      " |          A set of labels (any orderable and hashable object) for each\n",
      " |          sample. If the `classes` parameter is set, `y` will not be\n",
      " |          iterated.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_indicator : array or CSR matrix, shape (n_samples, n_classes)\n",
      " |          A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in\n",
      " |          `y[i]`, and 0 otherwise.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### MinMaxScaler ################\n",
      "Help on class MinMaxScaler in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.MinMaxScaler = class MinMaxScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.MinMaxScaler(feature_range=(0, 1), *, copy=True)\n",
      " |  \n",
      " |  Transform features by scaling each feature to a given range.\n",
      " |  \n",
      " |  This estimator scales and translates each feature individually such\n",
      " |  that it is in the given range on the training set, e.g. between\n",
      " |  zero and one.\n",
      " |  \n",
      " |  The transformation is given by::\n",
      " |  \n",
      " |      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      " |      X_scaled = X_std * (max - min) + min\n",
      " |  \n",
      " |  where min, max = feature_range.\n",
      " |  \n",
      " |  This transformation is often used as an alternative to zero mean,\n",
      " |  unit variance scaling.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  feature_range : tuple (min, max), default=(0, 1)\n",
      " |      Desired range of transformed data.\n",
      " |  \n",
      " |  copy : bool, default=True\n",
      " |      Set to False to perform inplace row normalization and avoid a\n",
      " |      copy (if the input is already a numpy array).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  min_ : ndarray of shape (n_features,)\n",
      " |      Per feature adjustment for minimum. Equivalent to\n",
      " |      ``min - X.min(axis=0) * self.scale_``\n",
      " |  \n",
      " |  scale_ : ndarray of shape (n_features,)\n",
      " |      Per feature relative scaling of the data. Equivalent to\n",
      " |      ``(max - min) / (X.max(axis=0) - X.min(axis=0))``\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  data_min_ : ndarray of shape (n_features,)\n",
      " |      Per feature minimum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_min_*\n",
      " |  \n",
      " |  data_max_ : ndarray of shape (n_features,)\n",
      " |      Per feature maximum seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_max_*\n",
      " |  \n",
      " |  data_range_ : ndarray of shape (n_features,)\n",
      " |      Per feature range ``(data_max_ - data_min_)`` seen in the data\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *data_range_*\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator.\n",
      " |      It will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MinMaxScaler\n",
      " |  >>> data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
      " |  >>> scaler = MinMaxScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  MinMaxScaler()\n",
      " |  >>> print(scaler.data_max_)\n",
      " |  [ 1. 18.]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[0.   0.  ]\n",
      " |   [0.25 0.25]\n",
      " |   [0.5  0.5 ]\n",
      " |   [1.   1.  ]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[1.5 0. ]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  minmax_scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MinMaxScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, feature_range=(0, 1), *, copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the minimum and maximum to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the per-feature minimum and maximum\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted scaler.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Undo the scaling of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed. It cannot be sparse.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of min and max on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Transformer instance.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scale features of X according to feature_range.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Input data that will be transformed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : array-like of shape (n_samples, n_features)\n",
      " |          Transformed data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### MaxAbsScaler ################\n",
      "Help on class MaxAbsScaler in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.MaxAbsScaler = class MaxAbsScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.MaxAbsScaler(*, copy=True)\n",
      " |  \n",
      " |  Scale each feature by its maximum absolute value.\n",
      " |  \n",
      " |  This estimator scales and translates each feature individually such\n",
      " |  that the maximal absolute value of each feature in the\n",
      " |  training set will be 1.0. It does not shift/center the data, and\n",
      " |  thus does not destroy any sparsity.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices.\n",
      " |  \n",
      " |  .. versionadded:: 0.17\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : boolean, optional, default is True\n",
      " |      Set to False to perform inplace scaling and avoid a copy (if the input\n",
      " |      is already a numpy array).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray, shape (n_features,)\n",
      " |      Per feature relative scaling of the data.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  max_abs_ : ndarray, shape (n_features,)\n",
      " |      Per feature maximum absolute value.\n",
      " |  \n",
      " |  n_samples_seen_ : int\n",
      " |      The number of samples processed by the estimator. Will be reset on\n",
      " |      new calls to fit, but increments across ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import MaxAbsScaler\n",
      " |  >>> X = [[ 1., -1.,  2.],\n",
      " |  ...      [ 2.,  0.,  0.],\n",
      " |  ...      [ 0.,  1., -1.]]\n",
      " |  >>> transformer = MaxAbsScaler().fit(X)\n",
      " |  >>> transformer\n",
      " |  MaxAbsScaler()\n",
      " |  >>> transformer.transform(X)\n",
      " |  array([[ 0.5, -1. ,  1. ],\n",
      " |         [ 1. ,  0. ,  0. ],\n",
      " |         [ 0. ,  1. , -0.5]])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  maxabs_scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MaxAbsScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the maximum absolute value to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the per-feature minimum and maximum\n",
      " |          used for later scaling along the features axis.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}\n",
      " |          The data that should be transformed back.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of max absolute value of X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Transformer instance.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Scale the data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}\n",
      " |          The data that should be scaled.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### QuantileTransformer ################\n",
      "Help on class QuantileTransformer in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.QuantileTransformer = class QuantileTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.QuantileTransformer(*, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)\n",
      " |  \n",
      " |  Transform features using quantiles information.\n",
      " |  \n",
      " |  This method transforms the features to follow a uniform or a normal\n",
      " |  distribution. Therefore, for a given feature, this transformation tends\n",
      " |  to spread out the most frequent values. It also reduces the impact of\n",
      " |  (marginal) outliers: this is therefore a robust preprocessing scheme.\n",
      " |  \n",
      " |  The transformation is applied on each feature independently. First an\n",
      " |  estimate of the cumulative distribution function of a feature is\n",
      " |  used to map the original values to a uniform distribution. The obtained\n",
      " |  values are then mapped to the desired output distribution using the\n",
      " |  associated quantile function. Features values of new/unseen data that fall\n",
      " |  below or above the fitted range will be mapped to the bounds of the output\n",
      " |  distribution. Note that this transform is non-linear. It may distort linear\n",
      " |  correlations between variables measured at the same scale but renders\n",
      " |  variables measured at different scales more directly comparable.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.19\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_quantiles : int, optional (default=1000 or n_samples)\n",
      " |      Number of quantiles to be computed. It corresponds to the number\n",
      " |      of landmarks used to discretize the cumulative distribution function.\n",
      " |      If n_quantiles is larger than the number of samples, n_quantiles is set\n",
      " |      to the number of samples as a larger number of quantiles does not give\n",
      " |      a better approximation of the cumulative distribution function\n",
      " |      estimator.\n",
      " |  \n",
      " |  output_distribution : str, optional (default='uniform')\n",
      " |      Marginal distribution for the transformed data. The choices are\n",
      " |      'uniform' (default) or 'normal'.\n",
      " |  \n",
      " |  ignore_implicit_zeros : bool, optional (default=False)\n",
      " |      Only applies to sparse matrices. If True, the sparse entries of the\n",
      " |      matrix are discarded to compute the quantile statistics. If False,\n",
      " |      these entries are treated as zeros.\n",
      " |  \n",
      " |  subsample : int, optional (default=1e5)\n",
      " |      Maximum number of samples used to estimate the quantiles for\n",
      " |      computational efficiency. Note that the subsampling procedure may\n",
      " |      differ for value-identical sparse and dense matrices.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      Determines random number generation for subsampling and smoothing\n",
      " |      noise.\n",
      " |      Please see ``subsample`` for more details.\n",
      " |      Pass an int for reproducible results across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`\n",
      " |  \n",
      " |  copy : boolean, optional, (default=True)\n",
      " |      Set to False to perform inplace transformation and avoid a copy (if the\n",
      " |      input is already a numpy array).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_quantiles_ : integer\n",
      " |      The actual number of quantiles used to discretize the cumulative\n",
      " |      distribution function.\n",
      " |  \n",
      " |  quantiles_ : ndarray, shape (n_quantiles, n_features)\n",
      " |      The values corresponding the quantiles of reference.\n",
      " |  \n",
      " |  references_ : ndarray, shape(n_quantiles, )\n",
      " |      Quantiles of references.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.preprocessing import QuantileTransformer\n",
      " |  >>> rng = np.random.RandomState(0)\n",
      " |  >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n",
      " |  >>> qt = QuantileTransformer(n_quantiles=10, random_state=0)\n",
      " |  >>> qt.fit_transform(X)\n",
      " |  array([...])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  quantile_transform : Equivalent function without the estimator API.\n",
      " |  PowerTransformer : Perform mapping to a normal distribution using a power\n",
      " |      transform.\n",
      " |  StandardScaler : Perform standardization that is faster, but less robust\n",
      " |      to outliers.\n",
      " |  RobustScaler : Perform robust standardization that removes the influence\n",
      " |      of outliers but does not put outliers and inliers on the same scale.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      QuantileTransformer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the quantiles used for transforming.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray or sparse matrix, shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis. If a sparse\n",
      " |          matrix is provided, it will be converted into a sparse\n",
      " |          ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
      " |          nonnegative if `ignore_implicit_zeros` is False.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Back-projection to the original space.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray or sparse matrix, shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis. If a sparse\n",
      " |          matrix is provided, it will be converted into a sparse\n",
      " |          ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
      " |          nonnegative if `ignore_implicit_zeros` is False.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray or sparse matrix, shape (n_samples, n_features)\n",
      " |          The projected data.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Feature-wise transformation of the data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray or sparse matrix, shape (n_samples, n_features)\n",
      " |          The data used to scale along the features axis. If a sparse\n",
      " |          matrix is provided, it will be converted into a sparse\n",
      " |          ``csc_matrix``. Additionally, the sparse matrix needs to be\n",
      " |          nonnegative if `ignore_implicit_zeros` is False.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Xt : ndarray or sparse matrix, shape (n_samples, n_features)\n",
      " |          The projected data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### Normalizer ################\n",
      "Help on class Normalizer in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.Normalizer = class Normalizer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.Normalizer(norm='l2', *, copy=True)\n",
      " |  \n",
      " |  Normalize samples individually to unit norm.\n",
      " |  \n",
      " |  Each sample (i.e. each row of the data matrix) with at least one\n",
      " |  non zero component is rescaled independently of other samples so\n",
      " |  that its norm (l1, l2 or inf) equals one.\n",
      " |  \n",
      " |  This transformer is able to work both with dense numpy arrays and\n",
      " |  scipy.sparse matrix (use CSR format if you want to avoid the burden of\n",
      " |  a copy / conversion).\n",
      " |  \n",
      " |  Scaling inputs to unit norms is a common operation for text\n",
      " |  classification or clustering for instance. For instance the dot\n",
      " |  product of two l2-normalized TF-IDF vectors is the cosine similarity\n",
      " |  of the vectors and is the base similarity metric for the Vector\n",
      " |  Space Model commonly used by the Information Retrieval community.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_normalization>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  norm : 'l1', 'l2', or 'max', optional ('l2' by default)\n",
      " |      The norm to use to normalize each non zero sample. If norm='max'\n",
      " |      is used, values will be rescaled by the maximum of the absolute\n",
      " |      values.\n",
      " |  \n",
      " |  copy : boolean, optional, default True\n",
      " |      set to False to perform inplace row normalization and avoid a\n",
      " |      copy (if the input is already a numpy array or a scipy.sparse\n",
      " |      CSR matrix).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import Normalizer\n",
      " |  >>> X = [[4, 1, 2, 2],\n",
      " |  ...      [1, 3, 9, 3],\n",
      " |  ...      [5, 7, 5, 1]]\n",
      " |  >>> transformer = Normalizer().fit(X)  # fit does nothing.\n",
      " |  >>> transformer\n",
      " |  Normalizer()\n",
      " |  >>> transformer.transform(X)\n",
      " |  array([[0.8, 0.2, 0.4, 0.4],\n",
      " |         [0.1, 0.3, 0.9, 0.3],\n",
      " |         [0.5, 0.7, 0.5, 0.1]])\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  This estimator is stateless (besides constructor parameters), the\n",
      " |  fit method does nothing but is useful when used in a pipeline.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  normalize: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Normalizer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, norm='l2', *, copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Do nothing and return the estimator unchanged\n",
      " |      \n",
      " |      This method is just there to implement the usual API and hence\n",
      " |      work in pipelines.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Scale each non zero row of X to unit norm\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data to normalize, row by row. scipy.sparse matrices should be\n",
      " |          in CSR format to avoid an un-necessary copy.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### OneHotEncoder ################\n",
      "Help on class OneHotEncoder in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.OneHotEncoder = class OneHotEncoder(_BaseEncoder)\n",
      " |  sklearn.preprocessing.OneHotEncoder(*, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
      " |  \n",
      " |  Encode categorical features as a one-hot numeric array.\n",
      " |  \n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are encoded using a one-hot (aka 'one-of-K' or 'dummy')\n",
      " |  encoding scheme. This creates a binary column for each category and\n",
      " |  returns a sparse matrix or dense array (depending on the ``sparse``\n",
      " |  parameter)\n",
      " |  \n",
      " |  By default, the encoder derives the categories based on the unique values\n",
      " |  in each feature. Alternatively, you can also specify the `categories`\n",
      " |  manually.\n",
      " |  \n",
      " |  This encoding is needed for feeding categorical data to many scikit-learn\n",
      " |  estimators, notably linear models and SVMs with the standard kernels.\n",
      " |  \n",
      " |  Note: a one-hot encoding of y labels should use a LabelBinarizer\n",
      " |  instead.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  \n",
      " |  .. versionchanged:: 0.20\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of array-like, default='auto'\n",
      " |      Categories (unique values) per feature:\n",
      " |  \n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values within a single feature, and should be sorted in case of\n",
      " |        numeric values.\n",
      " |  \n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  drop : {'first', 'if_binary'} or a array-like of shape (n_features,),             default=None\n",
      " |      Specifies a methodology to use to drop one of the categories per\n",
      " |      feature. This is useful in situations where perfectly collinear\n",
      " |      features cause problems, such as when feeding the resulting data\n",
      " |      into a neural network or an unregularized regression.\n",
      " |  \n",
      " |      However, dropping one category breaks the symmetry of the original\n",
      " |      representation and can therefore induce a bias in downstream models,\n",
      " |      for instance for penalized linear classification or regression models.\n",
      " |  \n",
      " |      - None : retain all features (the default).\n",
      " |      - 'first' : drop the first category in each feature. If only one\n",
      " |        category is present, the feature will be dropped entirely.\n",
      " |      - 'if_binary' : drop the first category in each feature with two\n",
      " |        categories. Features with 1 or more than 2 categories are\n",
      " |        left intact.\n",
      " |      - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n",
      " |        should be dropped.\n",
      " |  \n",
      " |  sparse : bool, default=True\n",
      " |      Will return sparse matrix if set True else will return an array.\n",
      " |  \n",
      " |  dtype : number type, default=np.float\n",
      " |      Desired dtype of output.\n",
      " |  \n",
      " |  handle_unknown : {'error', 'ignore'}, default='error'\n",
      " |      Whether to raise an error or ignore if an unknown categorical feature\n",
      " |      is present during transform (default is to raise). When this parameter\n",
      " |      is set to 'ignore' and an unknown category is encountered during\n",
      " |      transform, the resulting one-hot encoded columns for this feature\n",
      " |      will be all zeros. In the inverse transform, an unknown category\n",
      " |      will be denoted as None.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during fitting\n",
      " |      (in order of the features in X and corresponding with the output\n",
      " |      of ``transform``). This includes the category specified in ``drop``\n",
      " |      (if any).\n",
      " |  \n",
      " |  drop_idx_ : array of shape (n_features,)\n",
      " |      - ``drop_idx_[i]`` is the index in ``categories_[i]`` of the category\n",
      " |        to be dropped for each feature.\n",
      " |      - ``drop_idx_[i] = None`` if no category is to be dropped from the\n",
      " |        feature with index ``i``, e.g. when `drop='if_binary'` and the\n",
      " |        feature isn't binary.\n",
      " |      - ``drop_idx_ = None`` if all the transformed features will be\n",
      " |        retained.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OrdinalEncoder : Performs an ordinal (integer)\n",
      " |    encoding of the categorical features.\n",
      " |  sklearn.feature_extraction.DictVectorizer : Performs a one-hot encoding of\n",
      " |    dictionary items (also handles string-valued features).\n",
      " |  sklearn.feature_extraction.FeatureHasher : Performs an approximate one-hot\n",
      " |    encoding of dictionary items or strings.\n",
      " |  sklearn.preprocessing.LabelBinarizer : Binarizes labels in a one-vs-all\n",
      " |    fashion.\n",
      " |  sklearn.preprocessing.MultiLabelBinarizer : Transforms between iterable of\n",
      " |    iterables and a multilabel format, e.g. a (samples x classes) binary\n",
      " |    matrix indicating the presence of a class label.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to a binary one-hot encoding.\n",
      " |  \n",
      " |  >>> from sklearn.preprocessing import OneHotEncoder\n",
      " |  \n",
      " |  One can discard categories not seen during `fit`:\n",
      " |  \n",
      " |  >>> enc = OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  OneHotEncoder(handle_unknown='ignore')\n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 1], ['Male', 4]]).toarray()\n",
      " |  array([[1., 0., 1., 0., 0.],\n",
      " |         [0., 1., 0., 0., 0.]])\n",
      " |  >>> enc.inverse_transform([[0, 1, 1, 0, 0], [0, 0, 0, 1, 0]])\n",
      " |  array([['Male', 1],\n",
      " |         [None, 2]], dtype=object)\n",
      " |  >>> enc.get_feature_names(['gender', 'group'])\n",
      " |  array(['gender_Female', 'gender_Male', 'group_1', 'group_2', 'group_3'],\n",
      " |    dtype=object)\n",
      " |  \n",
      " |  One can always drop the first column for each feature:\n",
      " |  \n",
      " |  >>> drop_enc = OneHotEncoder(drop='first').fit(X)\n",
      " |  >>> drop_enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> drop_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 0., 0.],\n",
      " |         [1., 1., 0.]])\n",
      " |  \n",
      " |  Or drop a column for feature only having 2 categories:\n",
      " |  \n",
      " |  >>> drop_binary_enc = OneHotEncoder(drop='if_binary').fit(X)\n",
      " |  >>> drop_binary_enc.transform([['Female', 1], ['Male', 2]]).toarray()\n",
      " |  array([[0., 1., 0., 0.],\n",
      " |         [1., 0., 1., 0.]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OneHotEncoder\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, categories='auto', drop=None, sparse=True, dtype=<class 'numpy.float64'>, handle_unknown='error')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to determine the categories of each feature.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit OneHotEncoder to X, then transform X.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X) but more convenient.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : sparse matrix if sparse=True else a 2-d array\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  get_feature_names(self, input_features=None)\n",
      " |      Return feature names for output features.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : list of str of shape (n_features,)\n",
      " |          String names for input features if available. By default,\n",
      " |          \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      output_feature_names : ndarray of shape (n_output_features,)\n",
      " |          Array of feature names.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the data back to the original representation.\n",
      " |      \n",
      " |      In case unknown categories are encountered (all zeros in the\n",
      " |      one-hot encoding), ``None`` is used to represent this category.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Inverse transformed array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X using one-hot encoding.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : sparse matrix if sparse=True else a 2-d array\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### OrdinalEncoder ################\n",
      "Help on class OrdinalEncoder in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.OrdinalEncoder = class OrdinalEncoder(_BaseEncoder)\n",
      " |  sklearn.preprocessing.OrdinalEncoder(*, categories='auto', dtype=<class 'numpy.float64'>)\n",
      " |  \n",
      " |  Encode categorical features as an integer array.\n",
      " |  \n",
      " |  The input to this transformer should be an array-like of integers or\n",
      " |  strings, denoting the values taken on by categorical (discrete) features.\n",
      " |  The features are converted to ordinal integers. This results in\n",
      " |  a single column of integers (0 to n_categories - 1) per feature.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.20\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  categories : 'auto' or a list of array-like, default='auto'\n",
      " |      Categories (unique values) per feature:\n",
      " |  \n",
      " |      - 'auto' : Determine categories automatically from the training data.\n",
      " |      - list : ``categories[i]`` holds the categories expected in the ith\n",
      " |        column. The passed categories should not mix strings and numeric\n",
      " |        values, and should be sorted in case of numeric values.\n",
      " |  \n",
      " |      The used categories can be found in the ``categories_`` attribute.\n",
      " |  \n",
      " |  dtype : number type, default np.float64\n",
      " |      Desired dtype of output.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  categories_ : list of arrays\n",
      " |      The categories of each feature determined during fitting\n",
      " |      (in order of the features in X and corresponding with the output\n",
      " |      of ``transform``).\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.preprocessing.OneHotEncoder : Performs a one-hot encoding of\n",
      " |    categorical features.\n",
      " |  sklearn.preprocessing.LabelEncoder : Encodes target labels with values\n",
      " |    between 0 and n_classes-1.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  Given a dataset with two features, we let the encoder find the unique\n",
      " |  values per feature and transform the data to an ordinal encoding.\n",
      " |  \n",
      " |  >>> from sklearn.preprocessing import OrdinalEncoder\n",
      " |  >>> enc = OrdinalEncoder()\n",
      " |  >>> X = [['Male', 1], ['Female', 3], ['Female', 2]]\n",
      " |  >>> enc.fit(X)\n",
      " |  OrdinalEncoder()\n",
      " |  >>> enc.categories_\n",
      " |  [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)]\n",
      " |  >>> enc.transform([['Female', 3], ['Male', 1]])\n",
      " |  array([[0., 2.],\n",
      " |         [1., 0.]])\n",
      " |  \n",
      " |  >>> enc.inverse_transform([[1, 0], [0, 1]])\n",
      " |  array([['Male', 1],\n",
      " |         ['Female', 2]], dtype=object)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OrdinalEncoder\n",
      " |      _BaseEncoder\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, categories='auto', dtype=<class 'numpy.float64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the OrdinalEncoder to X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to determine the categories of each feature.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored. This parameter exists only for compatibility with\n",
      " |          :class:`sklearn.pipeline.Pipeline`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Convert the data back to the original representation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Inverse transformed array.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X to ordinal codes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data to encode.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_out : sparse matrix or a 2-d array\n",
      " |          Transformed input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### PowerTransformer ################\n",
      "Help on class PowerTransformer in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.PowerTransformer = class PowerTransformer(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.PowerTransformer(method='yeo-johnson', *, standardize=True, copy=True)\n",
      " |  \n",
      " |  Apply a power transform featurewise to make data more Gaussian-like.\n",
      " |  \n",
      " |  Power transforms are a family of parametric, monotonic transformations\n",
      " |  that are applied to make data more Gaussian-like. This is useful for\n",
      " |  modeling issues related to heteroscedasticity (non-constant variance),\n",
      " |  or other situations where normality is desired.\n",
      " |  \n",
      " |  Currently, PowerTransformer supports the Box-Cox transform and the\n",
      " |  Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n",
      " |  minimizing skewness is estimated through maximum likelihood.\n",
      " |  \n",
      " |  Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n",
      " |  supports both positive or negative data.\n",
      " |  \n",
      " |  By default, zero-mean, unit-variance normalization is applied to the\n",
      " |  transformed data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.20\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  method : str, (default='yeo-johnson')\n",
      " |      The power transform method. Available methods are:\n",
      " |  \n",
      " |      - 'yeo-johnson' [1]_, works with positive and negative values\n",
      " |      - 'box-cox' [2]_, only works with strictly positive values\n",
      " |  \n",
      " |  standardize : boolean, default=True\n",
      " |      Set to True to apply zero-mean, unit-variance normalization to the\n",
      " |      transformed output.\n",
      " |  \n",
      " |  copy : boolean, optional, default=True\n",
      " |      Set to False to perform inplace computation during transformation.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  lambdas_ : array of float, shape (n_features,)\n",
      " |      The parameters of the power transformation for the selected features.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.preprocessing import PowerTransformer\n",
      " |  >>> pt = PowerTransformer()\n",
      " |  >>> data = [[1, 2], [3, 2], [4, 5]]\n",
      " |  >>> print(pt.fit(data))\n",
      " |  PowerTransformer()\n",
      " |  >>> print(pt.lambdas_)\n",
      " |  [ 1.386... -3.100...]\n",
      " |  >>> print(pt.transform(data))\n",
      " |  [[-1.316... -0.707...]\n",
      " |   [ 0.209... -0.707...]\n",
      " |   [ 1.106...  1.414...]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  power_transform : Equivalent function without the estimator API.\n",
      " |  \n",
      " |  QuantileTransformer : Maps data to a standard normal distribution with\n",
      " |      the parameter `output_distribution='normal'`.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in ``fit``, and maintained\n",
      " |  in ``transform``.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n",
      " |         improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n",
      " |         (2000).\n",
      " |  \n",
      " |  .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n",
      " |         of the Royal Statistical Society B, 26, 211-252 (1964).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PowerTransformer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, method='yeo-johnson', *, standardize=True, copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Estimate the optimal parameter lambda for each feature.\n",
      " |      \n",
      " |      The optimal lambda parameter for minimizing skewness is estimated on\n",
      " |      each feature independently using maximum likelihood.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The data used to estimate the optimal transformation parameters.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Apply the inverse power transformation using the fitted lambdas.\n",
      " |      \n",
      " |      The inverse of the Box-Cox transformation is given by::\n",
      " |      \n",
      " |          if lambda_ == 0:\n",
      " |              X = exp(X_trans)\n",
      " |          else:\n",
      " |              X = (X_trans * lambda_ + 1) ** (1 / lambda_)\n",
      " |      \n",
      " |      The inverse of the Yeo-Johnson transformation is given by::\n",
      " |      \n",
      " |          if X >= 0 and lambda_ == 0:\n",
      " |              X = exp(X_trans) - 1\n",
      " |          elif X >= 0 and lambda_ != 0:\n",
      " |              X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1\n",
      " |          elif X < 0 and lambda_ != 2:\n",
      " |              X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))\n",
      " |          elif X < 0 and lambda_ == 2:\n",
      " |              X = 1 - exp(-X_trans)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The transformed data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The original data\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Apply the power transform to each feature using the fitted lambdas.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The data to be transformed using a power transformation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_trans : array-like, shape (n_samples, n_features)\n",
      " |          The transformed data.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### RobustScaler ################\n",
      "Help on class RobustScaler in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.RobustScaler = class RobustScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.RobustScaler(*, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n",
      " |  \n",
      " |  Scale features using statistics that are robust to outliers.\n",
      " |  \n",
      " |  This Scaler removes the median and scales the data according to\n",
      " |  the quantile range (defaults to IQR: Interquartile Range).\n",
      " |  The IQR is the range between the 1st quartile (25th quantile)\n",
      " |  and the 3rd quartile (75th quantile).\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by\n",
      " |  computing the relevant statistics on the samples in the training\n",
      " |  set. Median and interquartile range are then stored to be used on\n",
      " |  later data using the ``transform`` method.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators. Typically this is done by removing the mean\n",
      " |  and scaling to unit variance. However, outliers can often influence the\n",
      " |  sample mean / variance in a negative way. In such cases, the median and\n",
      " |  the interquartile range often give better results.\n",
      " |  \n",
      " |  .. versionadded:: 0.17\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  with_centering : boolean, True by default\n",
      " |      If True, center the data before scaling.\n",
      " |      This will cause ``transform`` to raise an exception when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_scaling : boolean, True by default\n",
      " |      If True, scale the data to interquartile range.\n",
      " |  \n",
      " |  quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0\n",
      " |      Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR\n",
      " |      Quantile range used to calculate ``scale_``.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  copy : boolean, optional, default is True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  center_ : array of floats\n",
      " |      The median value for each feature in the training set.\n",
      " |  \n",
      " |  scale_ : array of floats\n",
      " |      The (scaled) interquartile range for each feature in the training set.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_* attribute.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import RobustScaler\n",
      " |  >>> X = [[ 1., -2.,  2.],\n",
      " |  ...      [ -2.,  1.,  3.],\n",
      " |  ...      [ 4.,  1., -2.]]\n",
      " |  >>> transformer = RobustScaler().fit(X)\n",
      " |  >>> transformer\n",
      " |  RobustScaler()\n",
      " |  >>> transformer.transform(X)\n",
      " |  array([[ 0. , -2. ,  0. ],\n",
      " |         [-1. ,  0. ,  0.4],\n",
      " |         [ 1. ,  0. , -1.6]])\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  robust_scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`sklearn.decomposition.PCA`\n",
      " |      Further removes the linear correlation across features with\n",
      " |      'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/Median\n",
      " |  https://en.wikipedia.org/wiki/Interquartile_range\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RobustScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the median and quantiles to be used for scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to compute the median and quantiles\n",
      " |          used for later scaling along the features axis.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like\n",
      " |          The data used to scale along the specified axis.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Center and scale the data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}\n",
      " |          The data used to scale along the specified axis.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### StandardScaler ################\n",
      "Help on class StandardScaler in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.StandardScaler = class StandardScaler(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n",
      " |  \n",
      " |  Standardize features by removing the mean and scaling to unit variance\n",
      " |  \n",
      " |  The standard score of a sample `x` is calculated as:\n",
      " |  \n",
      " |      z = (x - u) / s\n",
      " |  \n",
      " |  where `u` is the mean of the training samples or zero if `with_mean=False`,\n",
      " |  and `s` is the standard deviation of the training samples or one if\n",
      " |  `with_std=False`.\n",
      " |  \n",
      " |  Centering and scaling happen independently on each feature by computing\n",
      " |  the relevant statistics on the samples in the training set. Mean and\n",
      " |  standard deviation are then stored to be used on later data using\n",
      " |  :meth:`transform`.\n",
      " |  \n",
      " |  Standardization of a dataset is a common requirement for many\n",
      " |  machine learning estimators: they might behave badly if the\n",
      " |  individual features do not more or less look like standard normally\n",
      " |  distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
      " |  \n",
      " |  For instance many elements used in the objective function of\n",
      " |  a learning algorithm (such as the RBF kernel of Support Vector\n",
      " |  Machines or the L1 and L2 regularizers of linear models) assume that\n",
      " |  all features are centered around 0 and have variance in the same\n",
      " |  order. If a feature has a variance that is orders of magnitude larger\n",
      " |  that others, it might dominate the objective function and make the\n",
      " |  estimator unable to learn from other features correctly as expected.\n",
      " |  \n",
      " |  This scaler can also be applied to sparse CSR or CSC matrices by passing\n",
      " |  `with_mean=False` to avoid breaking the sparsity structure of the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  copy : boolean, optional, default True\n",
      " |      If False, try to avoid a copy and do inplace scaling instead.\n",
      " |      This is not guaranteed to always work inplace; e.g. if the data is\n",
      " |      not a NumPy array or scipy.sparse CSR matrix, a copy may still be\n",
      " |      returned.\n",
      " |  \n",
      " |  with_mean : boolean, True by default\n",
      " |      If True, center the data before scaling.\n",
      " |      This does not work (and will raise an exception) when attempted on\n",
      " |      sparse matrices, because centering them entails building a dense\n",
      " |      matrix which in common use cases is likely to be too large to fit in\n",
      " |      memory.\n",
      " |  \n",
      " |  with_std : boolean, True by default\n",
      " |      If True, scale the data to unit variance (or equivalently,\n",
      " |      unit standard deviation).\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  scale_ : ndarray or None, shape (n_features,)\n",
      " |      Per feature relative scaling of the data. This is calculated using\n",
      " |      `np.sqrt(var_)`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *scale_*\n",
      " |  \n",
      " |  mean_ : ndarray or None, shape (n_features,)\n",
      " |      The mean value for each feature in the training set.\n",
      " |      Equal to ``None`` when ``with_mean=False``.\n",
      " |  \n",
      " |  var_ : ndarray or None, shape (n_features,)\n",
      " |      The variance for each feature in the training set. Used to compute\n",
      " |      `scale_`. Equal to ``None`` when ``with_std=False``.\n",
      " |  \n",
      " |  n_samples_seen_ : int or array, shape (n_features,)\n",
      " |      The number of samples processed by the estimator for each feature.\n",
      " |      If there are not missing samples, the ``n_samples_seen`` will be an\n",
      " |      integer, otherwise it will be an array.\n",
      " |      Will be reset on new calls to fit, but increments across\n",
      " |      ``partial_fit`` calls.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
      " |  >>> scaler = StandardScaler()\n",
      " |  >>> print(scaler.fit(data))\n",
      " |  StandardScaler()\n",
      " |  >>> print(scaler.mean_)\n",
      " |  [0.5 0.5]\n",
      " |  >>> print(scaler.transform(data))\n",
      " |  [[-1. -1.]\n",
      " |   [-1. -1.]\n",
      " |   [ 1.  1.]\n",
      " |   [ 1.  1.]]\n",
      " |  >>> print(scaler.transform([[2, 2]]))\n",
      " |  [[3. 3.]]\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  scale: Equivalent function without the estimator API.\n",
      " |  \n",
      " |  :class:`sklearn.decomposition.PCA`\n",
      " |      Further removes the linear correlation across features with 'whiten=True'.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      " |  transform.\n",
      " |  \n",
      " |  We use a biased estimator for the standard deviation, equivalent to\n",
      " |  `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      " |  affect model performance.\n",
      " |  \n",
      " |  For a comparison of the different scalers, transformers, and normalizers,\n",
      " |  see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      " |  <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StandardScaler\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, copy=True, with_mean=True, with_std=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute the mean and std to be used for later scaling.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y\n",
      " |          Ignored\n",
      " |  \n",
      " |  inverse_transform(self, X, copy=None)\n",
      " |      Scale back the data to the original representation\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_tr : array-like, shape [n_samples, n_features]\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  partial_fit(self, X, y=None)\n",
      " |      Online computation of mean and std on X for later scaling.\n",
      " |      \n",
      " |      All of X is processed as a single batch. This is intended for cases\n",
      " |      when :meth:`fit` is not feasible due to very large number of\n",
      " |      `n_samples` or because X is read from a continuous stream.\n",
      " |      \n",
      " |      The algorithm for incremental mean and std is given in Equation 1.5a,b\n",
      " |      in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\n",
      " |      for computing the sample variance: Analysis and recommendations.\"\n",
      " |      The American Statistician 37.3 (1983): 242-247:\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      " |          The data used to compute the mean and standard deviation\n",
      " |          used for later scaling along the features axis.\n",
      " |      \n",
      " |      y : None\n",
      " |          Ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Transformer instance.\n",
      " |  \n",
      " |  transform(self, X, copy=None)\n",
      " |      Perform standardization by centering and scaling\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape [n_samples, n_features]\n",
      " |          The data used to scale along the features axis.\n",
      " |      copy : bool, optional (default: None)\n",
      " |          Copy the input X or not.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### add_dummy_feature ################\n",
      "Help on function add_dummy_feature in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.add_dummy_feature = add_dummy_feature(X, value=1.0)\n",
      "    Augment dataset with an additional dummy feature.\n",
      "    \n",
      "    This is useful for fitting an intercept term with implementations which\n",
      "    cannot otherwise fit it directly.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      "        Data.\n",
      "    \n",
      "    value : float\n",
      "        Value to use for the dummy feature.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    \n",
      "    X : {array, sparse matrix}, shape [n_samples, n_features + 1]\n",
      "        Same data with dummy feature added as first column.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    >>> from sklearn.preprocessing import add_dummy_feature\n",
      "    >>> add_dummy_feature([[0, 1], [1, 0]])\n",
      "    array([[1., 0., 1.],\n",
      "           [1., 1., 0.]])\n",
      "\n",
      "None\n",
      "############### PolynomialFeatures ################\n",
      "Help on class PolynomialFeatures in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.PolynomialFeatures = class PolynomialFeatures(sklearn.base.TransformerMixin, sklearn.base.BaseEstimator)\n",
      " |  sklearn.preprocessing.PolynomialFeatures(degree=2, *, interaction_only=False, include_bias=True, order='C')\n",
      " |  \n",
      " |  Generate polynomial and interaction features.\n",
      " |  \n",
      " |  Generate a new feature matrix consisting of all polynomial combinations\n",
      " |  of the features with degree less than or equal to the specified degree.\n",
      " |  For example, if an input sample is two dimensional and of the form\n",
      " |  [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  degree : integer\n",
      " |      The degree of the polynomial features. Default = 2.\n",
      " |  \n",
      " |  interaction_only : boolean, default = False\n",
      " |      If true, only interaction features are produced: features that are\n",
      " |      products of at most ``degree`` *distinct* input features (so not\n",
      " |      ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).\n",
      " |  \n",
      " |  include_bias : boolean\n",
      " |      If True (default), then include a bias column, the feature in which\n",
      " |      all polynomial powers are zero (i.e. a column of ones - acts as an\n",
      " |      intercept term in a linear model).\n",
      " |  \n",
      " |  order : str in {'C', 'F'}, default 'C'\n",
      " |      Order of output array in the dense case. 'F' order is faster to\n",
      " |      compute, but may slow down subsequent estimators.\n",
      " |  \n",
      " |      .. versionadded:: 0.21\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.preprocessing import PolynomialFeatures\n",
      " |  >>> X = np.arange(6).reshape(3, 2)\n",
      " |  >>> X\n",
      " |  array([[0, 1],\n",
      " |         [2, 3],\n",
      " |         [4, 5]])\n",
      " |  >>> poly = PolynomialFeatures(2)\n",
      " |  >>> poly.fit_transform(X)\n",
      " |  array([[ 1.,  0.,  1.,  0.,  0.,  1.],\n",
      " |         [ 1.,  2.,  3.,  4.,  6.,  9.],\n",
      " |         [ 1.,  4.,  5., 16., 20., 25.]])\n",
      " |  >>> poly = PolynomialFeatures(interaction_only=True)\n",
      " |  >>> poly.fit_transform(X)\n",
      " |  array([[ 1.,  0.,  1.,  0.],\n",
      " |         [ 1.,  2.,  3.,  6.],\n",
      " |         [ 1.,  4.,  5., 20.]])\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  powers_ : array, shape (n_output_features, n_input_features)\n",
      " |      powers_[i, j] is the exponent of the jth input in the ith output.\n",
      " |  \n",
      " |  n_input_features_ : int\n",
      " |      The total number of input features.\n",
      " |  \n",
      " |  n_output_features_ : int\n",
      " |      The total number of polynomial output features. The number of output\n",
      " |      features is computed by iterating over all suitably sized combinations\n",
      " |      of input features.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  Be aware that the number of features in the output array scales\n",
      " |  polynomially in the number of features of the input array, and\n",
      " |  exponentially in the degree. High degrees can cause overfitting.\n",
      " |  \n",
      " |  See :ref:`examples/linear_model/plot_polynomial_interpolation.py\n",
      " |  <sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py>`\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PolynomialFeatures\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, degree=2, *, interaction_only=False, include_bias=True, order='C')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Compute number of output features.\n",
      " |      \n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : instance\n",
      " |  \n",
      " |  get_feature_names(self, input_features=None)\n",
      " |      Return feature names for output features\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_features : list of string, length n_features, optional\n",
      " |          String names for input features if available. By default,\n",
      " |          \"x0\", \"x1\", ... \"xn_features\" is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      output_feature_names : list of string, length n_output_features\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform data to polynomial features\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or CSR/CSC sparse matrix, shape [n_samples, n_features]\n",
      " |          The data to transform, row by row.\n",
      " |      \n",
      " |          Prefer CSR over CSC for sparse input (for speed), but CSC is\n",
      " |          required if the degree is 4 or higher. If the degree is less than\n",
      " |          4 and the input format is CSC, it will be converted to CSR, have\n",
      " |          its polynomial features generated, then converted back to CSC.\n",
      " |      \n",
      " |          If the degree is 2 or 3, the method described in \"Leveraging\n",
      " |          Sparsity to Speed Up Polynomial Feature Expansions of CSR Matrices\n",
      " |          Using K-Simplex Numbers\" by Andrew Nystrom and John Hughes is\n",
      " |          used, which is much faster than the method used on CSC input. For\n",
      " |          this reason, a CSC input will be converted to CSR, and the output\n",
      " |          will be converted back to CSC prior to being returned, hence the\n",
      " |          preference of CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      XP : np.ndarray or CSR/CSC sparse matrix, shape [n_samples, NP]\n",
      " |          The matrix of features, where NP is the number of polynomial\n",
      " |          features generated from the combination of inputs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  powers_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, **fit_params)\n",
      " |      Fit to data, then transform it.\n",
      " |      \n",
      " |      Fits transformer to X and y with optional parameters fit_params\n",
      " |      and returns a transformed version of X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,), default=None\n",
      " |          Target values.\n",
      " |      \n",
      " |      **fit_params : dict\n",
      " |          Additional fit parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      " |          Transformed array.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n",
      "############### binarize ################\n",
      "Help on function binarize in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.binarize = binarize(X, *, threshold=0.0, copy=True)\n",
      "    Boolean thresholding of array-like or scipy.sparse matrix\n",
      "    \n",
      "    Read more in the :ref:`User Guide <preprocessing_binarization>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      "        The data to binarize, element by element.\n",
      "        scipy.sparse matrices should be in CSR or CSC format to avoid an\n",
      "        un-necessary copy.\n",
      "    \n",
      "    threshold : float, optional (0.0 by default)\n",
      "        Feature values below or equal to this are replaced by 0, above it by 1.\n",
      "        Threshold may not be less than 0 for operations on sparse matrices.\n",
      "    \n",
      "    copy : boolean, optional, default True\n",
      "        set to False to perform inplace binarization and avoid a copy\n",
      "        (if the input is already a numpy array or a scipy.sparse CSR / CSC\n",
      "        matrix and if axis is 1).\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    Binarizer: Performs binarization using the ``Transformer`` API\n",
      "        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n",
      "\n",
      "None\n",
      "############### normalize ################\n",
      "Help on function normalize in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.normalize = normalize(X, norm='l2', *, axis=1, copy=True, return_norm=False)\n",
      "    Scale input vectors individually to unit norm (vector length).\n",
      "    \n",
      "    Read more in the :ref:`User Guide <preprocessing_normalization>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      "        The data to normalize, element by element.\n",
      "        scipy.sparse matrices should be in CSR format to avoid an\n",
      "        un-necessary copy.\n",
      "    \n",
      "    norm : 'l1', 'l2', or 'max', optional ('l2' by default)\n",
      "        The norm to use to normalize each non zero sample (or each non-zero\n",
      "        feature if axis is 0).\n",
      "    \n",
      "    axis : 0 or 1, optional (1 by default)\n",
      "        axis used to normalize the data along. If 1, independently normalize\n",
      "        each sample, otherwise (if 0) normalize each feature.\n",
      "    \n",
      "    copy : boolean, optional, default True\n",
      "        set to False to perform inplace row normalization and avoid a\n",
      "        copy (if the input is already a numpy array or a scipy.sparse\n",
      "        CSR matrix and if axis is 1).\n",
      "    \n",
      "    return_norm : boolean, default False\n",
      "        whether to return the computed norms\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X : {array-like, sparse matrix}, shape [n_samples, n_features]\n",
      "        Normalized input X.\n",
      "    \n",
      "    norms : array, shape [n_samples] if axis=1 else [n_features]\n",
      "        An array of norms along given axis for X.\n",
      "        When X is sparse, a NotImplementedError will be raised\n",
      "        for norm 'l1' or 'l2'.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    Normalizer: Performs normalization using the ``Transformer`` API\n",
      "        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    For a comparison of the different scalers, transformers, and normalizers,\n",
      "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "\n",
      "None\n",
      "############### scale ################\n",
      "Help on function scale in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.scale = scale(X, *, axis=0, with_mean=True, with_std=True, copy=True)\n",
      "    Standardize a dataset along any axis\n",
      "    \n",
      "    Center to the mean and component wise scale to unit variance.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : {array-like, sparse matrix}\n",
      "        The data to center and scale.\n",
      "    \n",
      "    axis : int (0 by default)\n",
      "        axis used to compute the means and standard deviations along. If 0,\n",
      "        independently standardize each feature, otherwise (if 1) standardize\n",
      "        each sample.\n",
      "    \n",
      "    with_mean : boolean, True by default\n",
      "        If True, center the data before scaling.\n",
      "    \n",
      "    with_std : boolean, True by default\n",
      "        If True, scale the data to unit variance (or equivalently,\n",
      "        unit standard deviation).\n",
      "    \n",
      "    copy : boolean, optional, default True\n",
      "        set to False to perform inplace row normalization and avoid a\n",
      "        copy (if the input is already a numpy array or a scipy.sparse\n",
      "        CSC matrix and if axis is 1).\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This implementation will refuse to center scipy.sparse matrices\n",
      "    since it would make them non-sparse and would potentially crash the\n",
      "    program with memory exhaustion problems.\n",
      "    \n",
      "    Instead the caller is expected to either set explicitly\n",
      "    `with_mean=False` (in that case, only variance scaling will be\n",
      "    performed on the features of the CSC matrix) or to call `X.toarray()`\n",
      "    if he/she expects the materialized dense array to fit in memory.\n",
      "    \n",
      "    To avoid memory copy the caller should pass a CSC matrix.\n",
      "    \n",
      "    NaNs are treated as missing values: disregarded to compute the statistics,\n",
      "    and maintained during the data transformation.\n",
      "    \n",
      "    We use a biased estimator for the standard deviation, equivalent to\n",
      "    `numpy.std(x, ddof=0)`. Note that the choice of `ddof` is unlikely to\n",
      "    affect model performance.\n",
      "    \n",
      "    For a comparison of the different scalers, transformers, and normalizers,\n",
      "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    StandardScaler: Performs scaling to unit variance using the``Transformer`` API\n",
      "        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n",
      "\n",
      "None\n",
      "############### robust_scale ################\n",
      "Help on function robust_scale in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.robust_scale = robust_scale(X, *, axis=0, with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)\n",
      "    Standardize a dataset along any axis\n",
      "    \n",
      "    Center to the median and component wise scale\n",
      "    according to the interquartile range.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like\n",
      "        The data to center and scale.\n",
      "    \n",
      "    axis : int (0 by default)\n",
      "        axis used to compute the medians and IQR along. If 0,\n",
      "        independently scale each feature, otherwise (if 1) scale\n",
      "        each sample.\n",
      "    \n",
      "    with_centering : boolean, True by default\n",
      "        If True, center the data before scaling.\n",
      "    \n",
      "    with_scaling : boolean, True by default\n",
      "        If True, scale the data to unit variance (or equivalently,\n",
      "        unit standard deviation).\n",
      "    \n",
      "    quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0\n",
      "        Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR\n",
      "        Quantile range used to calculate ``scale_``.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    copy : boolean, optional, default is True\n",
      "        set to False to perform inplace row normalization and avoid a\n",
      "        copy (if the input is already a numpy array or a scipy.sparse\n",
      "        CSR matrix and if axis is 1).\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This implementation will refuse to center scipy.sparse matrices\n",
      "    since it would make them non-sparse and would potentially crash the\n",
      "    program with memory exhaustion problems.\n",
      "    \n",
      "    Instead the caller is expected to either set explicitly\n",
      "    `with_centering=False` (in that case, only variance scaling will be\n",
      "    performed on the features of the CSR matrix) or to call `X.toarray()`\n",
      "    if he/she expects the materialized dense array to fit in memory.\n",
      "    \n",
      "    To avoid memory copy the caller should pass a CSR matrix.\n",
      "    \n",
      "    For a comparison of the different scalers, transformers, and normalizers,\n",
      "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    RobustScaler: Performs centering and scaling using the ``Transformer`` API\n",
      "        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n",
      "\n",
      "None\n",
      "############### maxabs_scale ################\n",
      "Help on function maxabs_scale in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.maxabs_scale = maxabs_scale(X, *, axis=0, copy=True)\n",
      "    Scale each feature to the [-1, 1] range without breaking the sparsity.\n",
      "    \n",
      "    This estimator scales each feature individually such\n",
      "    that the maximal absolute value of each feature in the\n",
      "    training set will be 1.0.\n",
      "    \n",
      "    This scaler can also be applied to sparse CSR or CSC matrices.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        The data.\n",
      "    \n",
      "    axis : int (0 by default)\n",
      "        axis used to scale along. If 0, independently scale each feature,\n",
      "        otherwise (if 1) scale each sample.\n",
      "    \n",
      "    copy : boolean, optional, default is True\n",
      "        Set to False to perform inplace scaling and avoid a copy (if the input\n",
      "        is already a numpy array).\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    MaxAbsScaler: Performs scaling to the [-1, 1] range using the``Transformer`` API\n",
      "        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    NaNs are treated as missing values: disregarded to compute the statistics,\n",
      "    and maintained during the data transformation.\n",
      "    \n",
      "    For a comparison of the different scalers, transformers, and normalizers,\n",
      "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "\n",
      "None\n",
      "############### minmax_scale ################\n",
      "Help on function minmax_scale in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.minmax_scale = minmax_scale(X, feature_range=(0, 1), *, axis=0, copy=True)\n",
      "    Transform features by scaling each feature to a given range.\n",
      "    \n",
      "    This estimator scales and translates each feature individually such\n",
      "    that it is in the given range on the training set, i.e. between\n",
      "    zero and one.\n",
      "    \n",
      "    The transformation is given by (when ``axis=0``)::\n",
      "    \n",
      "        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
      "        X_scaled = X_std * (max - min) + min\n",
      "    \n",
      "    where min, max = feature_range.\n",
      "    \n",
      "    The transformation is calculated as (when ``axis=0``)::\n",
      "    \n",
      "       X_scaled = scale * X + min - X.min(axis=0) * scale\n",
      "       where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n",
      "    \n",
      "    This transformation is often used as an alternative to zero mean,\n",
      "    unit variance scaling.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "    \n",
      "    .. versionadded:: 0.17\n",
      "       *minmax_scale* function interface\n",
      "       to :class:`sklearn.preprocessing.MinMaxScaler`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like of shape (n_samples, n_features)\n",
      "        The data.\n",
      "    \n",
      "    feature_range : tuple (min, max), default=(0, 1)\n",
      "        Desired range of transformed data.\n",
      "    \n",
      "    axis : int, default=0\n",
      "        Axis used to scale along. If 0, independently scale each feature,\n",
      "        otherwise (if 1) scale each sample.\n",
      "    \n",
      "    copy : bool, default=True\n",
      "        Set to False to perform inplace scaling and avoid a copy (if the input\n",
      "        is already a numpy array).\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    MinMaxScaler: Performs scaling to a given range using the``Transformer`` API\n",
      "        (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`).\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    For a comparison of the different scalers, transformers, and normalizers,\n",
      "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "\n",
      "None\n",
      "############### label_binarize ################\n",
      "Help on function label_binarize in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.label_binarize = label_binarize(y, *, classes, neg_label=0, pos_label=1, sparse_output=False)\n",
      "    Binarize labels in a one-vs-all fashion\n",
      "    \n",
      "    Several regression and binary classification algorithms are\n",
      "    available in scikit-learn. A simple way to extend these algorithms\n",
      "    to the multi-class classification case is to use the so-called\n",
      "    one-vs-all scheme.\n",
      "    \n",
      "    This function makes it possible to compute this transformation for a\n",
      "    fixed set of class labels known ahead of time.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y : array-like\n",
      "        Sequence of integer labels or multilabel data to encode.\n",
      "    \n",
      "    classes : array-like of shape [n_classes]\n",
      "        Uniquely holds the label for each class.\n",
      "    \n",
      "    neg_label : int (default: 0)\n",
      "        Value with which negative labels must be encoded.\n",
      "    \n",
      "    pos_label : int (default: 1)\n",
      "        Value with which positive labels must be encoded.\n",
      "    \n",
      "    sparse_output : boolean (default: False),\n",
      "        Set to true if output binary array is desired in CSR sparse format\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Y : numpy array or CSR matrix of shape [n_samples, n_classes]\n",
      "        Shape will be [n_samples, 1] for binary problems.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.preprocessing import label_binarize\n",
      "    >>> label_binarize([1, 6], classes=[1, 2, 4, 6])\n",
      "    array([[1, 0, 0, 0],\n",
      "           [0, 0, 0, 1]])\n",
      "    \n",
      "    The class ordering is preserved:\n",
      "    \n",
      "    >>> label_binarize([1, 6], classes=[1, 6, 4, 2])\n",
      "    array([[1, 0, 0, 0],\n",
      "           [0, 1, 0, 0]])\n",
      "    \n",
      "    Binary targets transform to a column vector\n",
      "    \n",
      "    >>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\n",
      "    array([[1],\n",
      "           [0],\n",
      "           [0],\n",
      "           [1]])\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    LabelBinarizer : class used to wrap the functionality of label_binarize and\n",
      "        allow for fitting to classes independently of the transform operation\n",
      "\n",
      "None\n",
      "############### quantile_transform ################\n",
      "Help on function quantile_transform in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.quantile_transform = quantile_transform(X, *, axis=0, n_quantiles=1000, output_distribution='uniform', ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)\n",
      "    Transform features using quantiles information.\n",
      "    \n",
      "    This method transforms the features to follow a uniform or a normal\n",
      "    distribution. Therefore, for a given feature, this transformation tends\n",
      "    to spread out the most frequent values. It also reduces the impact of\n",
      "    (marginal) outliers: this is therefore a robust preprocessing scheme.\n",
      "    \n",
      "    The transformation is applied on each feature independently. First an\n",
      "    estimate of the cumulative distribution function of a feature is\n",
      "    used to map the original values to a uniform distribution. The obtained\n",
      "    values are then mapped to the desired output distribution using the\n",
      "    associated quantile function. Features values of new/unseen data that fall\n",
      "    below or above the fitted range will be mapped to the bounds of the output\n",
      "    distribution. Note that this transform is non-linear. It may distort linear\n",
      "    correlations between variables measured at the same scale but renders\n",
      "    variables measured at different scales more directly comparable.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, sparse matrix\n",
      "        The data to transform.\n",
      "    \n",
      "    axis : int, (default=0)\n",
      "        Axis used to compute the means and standard deviations along. If 0,\n",
      "        transform each feature, otherwise (if 1) transform each sample.\n",
      "    \n",
      "    n_quantiles : int, optional (default=1000 or n_samples)\n",
      "        Number of quantiles to be computed. It corresponds to the number\n",
      "        of landmarks used to discretize the cumulative distribution function.\n",
      "        If n_quantiles is larger than the number of samples, n_quantiles is set\n",
      "        to the number of samples as a larger number of quantiles does not give\n",
      "        a better approximation of the cumulative distribution function\n",
      "        estimator.\n",
      "    \n",
      "    output_distribution : str, optional (default='uniform')\n",
      "        Marginal distribution for the transformed data. The choices are\n",
      "        'uniform' (default) or 'normal'.\n",
      "    \n",
      "    ignore_implicit_zeros : bool, optional (default=False)\n",
      "        Only applies to sparse matrices. If True, the sparse entries of the\n",
      "        matrix are discarded to compute the quantile statistics. If False,\n",
      "        these entries are treated as zeros.\n",
      "    \n",
      "    subsample : int, optional (default=1e5)\n",
      "        Maximum number of samples used to estimate the quantiles for\n",
      "        computational efficiency. Note that the subsampling procedure may\n",
      "        differ for value-identical sparse and dense matrices.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        Determines random number generation for subsampling and smoothing\n",
      "        noise.\n",
      "        Please see ``subsample`` for more details.\n",
      "        Pass an int for reproducible results across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`\n",
      "    \n",
      "    copy : boolean, optional, (default=True)\n",
      "        Set to False to perform inplace transformation and avoid a copy (if the\n",
      "        input is already a numpy array). If True, a copy of `X` is transformed,\n",
      "        leaving the original `X` unchanged\n",
      "    \n",
      "        ..versionchanged:: 0.23\n",
      "            The default value of `copy` changed from False to True in 0.23.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Xt : ndarray or sparse matrix, shape (n_samples, n_features)\n",
      "        The transformed data.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.preprocessing import quantile_transform\n",
      "    >>> rng = np.random.RandomState(0)\n",
      "    >>> X = np.sort(rng.normal(loc=0.5, scale=0.25, size=(25, 1)), axis=0)\n",
      "    >>> quantile_transform(X, n_quantiles=10, random_state=0, copy=True)\n",
      "    array([...])\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    QuantileTransformer : Performs quantile-based scaling using the\n",
      "        ``Transformer`` API (e.g. as part of a preprocessing\n",
      "        :class:`sklearn.pipeline.Pipeline`).\n",
      "    power_transform : Maps data to a normal distribution using a\n",
      "        power transformation.\n",
      "    scale : Performs standardization that is faster, but less robust\n",
      "        to outliers.\n",
      "    robust_scale : Performs robust standardization that removes the influence\n",
      "        of outliers but does not put outliers and inliers on the same scale.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    NaNs are treated as missing values: disregarded in fit, and maintained in\n",
      "    transform.\n",
      "    \n",
      "    For a comparison of the different scalers, transformers, and normalizers,\n",
      "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "\n",
      "None\n",
      "############### power_transform ################\n",
      "Help on function power_transform in sklearn.preprocessing:\n",
      "\n",
      "sklearn.preprocessing.power_transform = power_transform(X, method='yeo-johnson', *, standardize=True, copy=True)\n",
      "    Power transforms are a family of parametric, monotonic transformations\n",
      "    that are applied to make data more Gaussian-like. This is useful for\n",
      "    modeling issues related to heteroscedasticity (non-constant variance),\n",
      "    or other situations where normality is desired.\n",
      "    \n",
      "    Currently, power_transform supports the Box-Cox transform and the\n",
      "    Yeo-Johnson transform. The optimal parameter for stabilizing variance and\n",
      "    minimizing skewness is estimated through maximum likelihood.\n",
      "    \n",
      "    Box-Cox requires input data to be strictly positive, while Yeo-Johnson\n",
      "    supports both positive or negative data.\n",
      "    \n",
      "    By default, zero-mean, unit-variance normalization is applied to the\n",
      "    transformed data.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <preprocessing_transformer>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape (n_samples, n_features)\n",
      "        The data to be transformed using a power transformation.\n",
      "    \n",
      "    method : {'yeo-johnson', 'box-cox'}, default='yeo-johnson'\n",
      "        The power transform method. Available methods are:\n",
      "    \n",
      "        - 'yeo-johnson' [1]_, works with positive and negative values\n",
      "        - 'box-cox' [2]_, only works with strictly positive values\n",
      "    \n",
      "        .. versionchanged:: 0.23\n",
      "            The default value of the `method` parameter changed from\n",
      "            'box-cox' to 'yeo-johnson' in 0.23.\n",
      "    \n",
      "    standardize : boolean, default=True\n",
      "        Set to True to apply zero-mean, unit-variance normalization to the\n",
      "        transformed output.\n",
      "    \n",
      "    copy : boolean, optional, default=True\n",
      "        Set to False to perform inplace computation during transformation.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X_trans : array-like, shape (n_samples, n_features)\n",
      "        The transformed data.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.preprocessing import power_transform\n",
      "    >>> data = [[1, 2], [3, 2], [4, 5]]\n",
      "    >>> print(power_transform(data, method='box-cox'))\n",
      "    [[-1.332... -0.707...]\n",
      "     [ 0.256... -0.707...]\n",
      "     [ 1.076...  1.414...]]\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    PowerTransformer : Equivalent transformation with the\n",
      "        ``Transformer`` API (e.g. as part of a preprocessing\n",
      "        :class:`sklearn.pipeline.Pipeline`).\n",
      "    \n",
      "    quantile_transform : Maps data to a standard normal distribution with\n",
      "        the parameter `output_distribution='normal'`.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    NaNs are treated as missing values: disregarded in ``fit``, and maintained\n",
      "    in ``transform``.\n",
      "    \n",
      "    For a comparison of the different scalers, transformers, and normalizers,\n",
      "    see :ref:`examples/preprocessing/plot_all_scaling.py\n",
      "    <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    \n",
      "    .. [1] I.K. Yeo and R.A. Johnson, \"A new family of power transformations to\n",
      "           improve normality or symmetry.\" Biometrika, 87(4), pp.954-959,\n",
      "           (2000).\n",
      "    \n",
      "    .. [2] G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal\n",
      "           of the Royal Statistical Society B, 26, 211-252 (1964).\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "listbox = sklearn.preprocessing.__all__\n",
    "for item in listbox:\n",
    "\tprint(f\"############### {item} ################\")\n",
    "\tprint(help(\"sklearn.preprocessing.\" + item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "278694bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T07:24:50.119469Z",
     "iopub.status.busy": "2025-06-19T07:24:50.118758Z",
     "iopub.status.idle": "2025-06-19T07:24:50.122057Z",
     "shell.execute_reply": "2025-06-19T07:24:50.121542Z",
     "shell.execute_reply.started": "2021-11-28T00:21:24.257703Z"
    },
    "papermill": {
     "duration": 0.033819,
     "end_time": "2025-06-19T07:24:50.122213",
     "exception": false,
     "start_time": "2025-06-19T07:24:50.088394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sklearn.model_selection\n",
    "# listbox = sklearn.model_selection.__all__\n",
    "# for item in listbox:\n",
    "# \tprint(f\"############### {item} ################\")\n",
    "# \tprint(help(\"sklearn.model_selection.\" + item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39c4ca",
   "metadata": {
    "papermill": {
     "duration": 0.026474,
     "end_time": "2025-06-19T07:24:50.174335",
     "exception": false,
     "start_time": "2025-06-19T07:24:50.147861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1633303,
     "sourceId": 12211878,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30145,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.364689,
   "end_time": "2025-06-19T07:24:50.811996",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-19T07:24:38.447307",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
